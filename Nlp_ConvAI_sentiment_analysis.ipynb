{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment: Sentiment Analysis with TAO and Riva\n",
    "Sentiment analysis is a type of text classification, a common NLP task. \n",
    "Using a pretrained language model, such as BERT, it is possible to train a text classification model to classify sentences among defined categories.  In the case of sentiment analysis, there are only two categories: positive and negative.\n",
    "\n",
    "<img src=\"images/assess/sentiment_analysis.png\">\n",
    "\n",
    "### Table of Contents\n",
    "[The Problem](#The-Problem)<br>\n",
    "[Scoring](#Scoring)<br>\n",
    "[Step 1: Prepare the Project](#Step-1:-Prepare-the-Project)<br>\n",
    "[Step 2: Train](#Step-2:-Train)<br>\n",
    "[Step 3: Infer and Evaluate](#Step-3:-Infer-and-Evaluate)<br>\n",
    "[Step 4: Export Custom Model](#Step-4:-Export-Custom-Model)<br>\n",
    "[Step 5: Build and Deploy with Riva](#Step-5:-Build-and-Deploy-with-Riva)<br>\n",
    "[Step 6: Start Riva Services](#Step-6:-Start-Riva-Services)<br>\n",
    "[Step 7: Submit You Assessment](#Step-7:-Submit-You-Assessment)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Dependencies\n",
    "The steps in this notebook assume that you have:\n",
    "\n",
    "1. **NGC Credentials**<br>Be sure you have added your NGC credential as described in the [NGC Setup notebook](003_Intro_NGC_Setup.ipynb).  If you have restarted the course instance, you will need to repeat this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91f9425a29df\n",
      "CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n"
     ]
    }
   ],
   "source": [
    "# Start fresh...\n",
    "# Clear Docker containers\n",
    "!docker kill $(docker ps -q)\n",
    "# Check for clean environment - this should be empty\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# The Problem\n",
    "\n",
    "### SST-2 Movie Reviews\n",
    "The [Stanford Sentiment Treebank v2 (SST-2)](https://nlp.stanford.edu/sentiment/index.html) dataset is a corpus with fully labeled (two classes: positive and negative) single sentences extracted from movie reviews. Your task is to train a model using the dataset and deploy it to Riva, where you can run inference using the Riva API.\n",
    "\n",
    "### Your Project\n",
    "You are provided with labeled training and validation datasets, `train_small.tsv` and `dev_small.tsv` for the project.  There is also a test set, `test.tsv`, for a final test of the model.  All datasets are contained in the `tao/data/SST-2` directory.  You can open any of these files to take a look at the actual data and format:\n",
    "* [train_small.tsv](tao/data/SST-2/train_small.tsv)\n",
    "* [dev_small.tsv](tao/data/SST-2/dev_small.tsv)\n",
    "* [test.tsv](tao/data/SST-2/test.tsv)\n",
    "\n",
    "Your assignment is to train a [text classification model](https://docs.nvidia.com/tao/tao-toolkit/text/nlp/text_classification.html) with TAO using the `tao text_classification` launch command. After training, you must export the custom model, then deploy it using Riva.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Scoring\n",
    "You will be assessed on your ability to effectively and efficiently train and deploy the model.  This coding assessment is worth 70 points, divided as follows:\n",
    "\n",
    "### Rubric\n",
    "\n",
    "| Step                    | Graded                                                 | FIXMEs?  | Points |\n",
    "|-------------------------|--------------------------------------------------------|----------|--------|\n",
    "| 1. Prepare the Project  | Specs and path definitions (spec files are present)    |  1       | 5      |\n",
    "| 2. Train                | Efficient training parameters (faster training)        |  5       | 15     |\n",
    "| 3. Infer and Evaluate   | Achieve good inference performance (F1 value >= 88)    |  0       | 10     |\n",
    "| 4. Export Custom Model  | Export for Riva (model exported in correct format)   |  1       | 12     |\n",
    "| 5. Build and Deploy     | Riva ServiceMaker (correct models built and loaded)  |  2       | 14     |\n",
    "| 6. Start Riva         | Riva Server (correct config; models run)             |  1       | 14     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although you are very capable at this point of building the project without any help at all, some scaffolding is provided, including specific names for variables.  This is for the benefit of the autograder, so please use these constructs for your assessment.  In addition, a copy of the latest output for your executed cells in some cases is saved in the `my_assessment` directory.  Along the way, there are a few opportunities to check your work to see if you are on the right track. \n",
    "\n",
    "Once you are confident that you've built a reliable model, follow the instructions for submission at the end of the notebook.\n",
    "\n",
    "### Resources and Hints\n",
    "\n",
    "* **[TAO User's Guide](https://docs.nvidia.com/tao/tao-toolkit/index.html)**<br>\n",
    "* **[Riva Speech Skills User's Guide](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/index.html)**<br>\n",
    "* **TAO Example**<br>\n",
    "Review what you've learned in the [NER Fine-Tuning](007_NLP_Finetune_NER.ipynb) notebook to train, infer, evaluate, and export a TAO model.  The `tao token_classification` commands are very similar to the `tao text_classification` commands.\n",
    "* **Riva Deployment Example**<br>\n",
    "Review what you've learned in the [NER Model Deployment with Riva](008_NLP_Deploy_NER.ipynb) notebook to build and deploy the model, as well as start the Riva server.\n",
    "* **AMP Optimization Level (trainer.amp_level):**<br>\n",
    "To use mixed precision, set AMP to 'O1' or 'O2'; to train without mixed precision, set it to 'O0'.\n",
    "* **Precision (trainer.precision):**<br>\n",
    "To speed up training, you can set the precision to 16 instead of the standard 32 with little or no loss in accuracy.\n",
    "* **Number of epochs (trainer.max_epochs):**<br>\n",
    "The project is designed so that you should achieve success on this dataset with only 2 epochs, but feel free to run more. On a Tesla T4, this takes 5-6 minutes if you run it efficiently!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 1: Prepare the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Project Paths (not graded)\n",
    "This block is complete, but feel free to add to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the TAO paths for the project\n",
    "##### TAO paths - source\n",
    "SOURCE_MOUNT=\"/dli/task/tao\"\n",
    "DESTINATION_MOUNT = \"/workspace/mount\"\n",
    "\n",
    "##### TAO paths - source\n",
    "# Define location of the SST-2 dataset\n",
    "DATA = SOURCE_MOUNT+'/data/SST-2'\n",
    "# Directory where the .riva model is stored\n",
    "EXPORT_MODEL_LOC = SOURCE_MOUNT + '/results/sst2/export'\n",
    "\n",
    "##### TAO paths - destination (from the perspective of the TAO Docker)\n",
    "# The path to the specification YAML \n",
    "SPECS_DIR = DESTINATION_MOUNT + '/specs'\n",
    "# The results are saved at this path by default\n",
    "RESULTS_DIR = DESTINATION_MOUNT + '/results'\n",
    "# The data are saved at this path by default\n",
    "DATA_DIR = DESTINATION_MOUNT + '/data'\n",
    "# The results are saved at this path by default\n",
    "MODELS_DIR = DESTINATION_MOUNT + '/models'\n",
    "\n",
    "# Set your encryption key, and use the same key for all commands. Please use \"tlt_encode\" if you'd like to deploy the models later with NVIDIA Riva.\n",
    "KEY = 'tlt_encode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Spec Files (graded)\n",
    "Complete the <i><strong style=\"color:green;\">#FIXME</strong></i> line(s) and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-05 05:22:19,854 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-03-05 05:22:19,964 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2022-03-05 05:22:23 experimental:27] Module <class 'nemo.collections.nlp.modules.common.megatron.megatron_bert.MegatronBertEncoder'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "INFO: Generating new fontManager, this may take some time...\n",
      "[NeMo I 2022-03-05 05:22:25 tlt_logging:20] Experiment configuration:\n",
      "    exp_manager:\n",
      "      task_name: download_specs\n",
      "      explicit_log_dir: /workspace/mount/results\n",
      "    source_data_dir: /opt/conda/lib/python3.8/site-packages/nlp/text_classification/experiment_specs\n",
      "    target_data_dir: /workspace/mount/specs/text_classification\n",
      "    workflow: nlp\n",
      "    \n",
      "[NeMo W 2022-03-05 05:22:25 exp_manager:26] Exp_manager is logging to `/workspace/mount/results``, but it already exists.\n",
      "[NeMo I 2022-03-05 05:22:25 download_specs:73] Default specification files for nlp downloaded to '/workspace/mount/specs/text_classification'\n",
      "[NeMo I 2022-03-05 05:22:25 download_specs:74] Experiment logs saved to '/workspace/mount/results'\n",
      "2022-03-05 05:22:26,371 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from shutil import rmtree\n",
    "\n",
    "# Delete the specs directory if it already exists\n",
    "folder = SOURCE_MOUNT + '/specs'\n",
    "if os.path.exists(folder):\n",
    "    rmtree(folder)\n",
    "\n",
    "# Get the text classification task spec files\n",
    "!tao text_classification download_specs\\\n",
    "    -o $SPECS_DIR/text_classification \\\n",
    "    -r $RESULTS_DIR \\\n",
    "    2>&1|tee my_assessment/step1.txt # DO NOT REMOVE THIS LINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 2: Train\n",
    "### Run the Trainer (graded)\n",
    "Review the `train.yaml` file you've just downloaded. Run the trainer in TAO and override YAML config values as necessary.\n",
    "\n",
    "Complete the <i><strong style=\"color:green;\">#FIXME</strong></i> line(s) and run the cell. Feel free to add/remove override values as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-05 05:43:15,881 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-03-05 05:43:15,996 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2022-03-05 05:43:19 experimental:27] Module <class 'nemo.collections.nlp.modules.common.megatron.megatron_bert.MegatronBertEncoder'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "INFO: Generating new fontManager, this may take some time...\n",
      "[NeMo W 2022-03-05 05:43:22 experimental:27] Module <class 'nemo.collections.nlp.modules.common.megatron.megatron_bert.MegatronBertEncoder'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo I 2022-03-05 05:43:23 tlt_logging:20] Experiment configuration:\n",
      "    restore_from: ???\n",
      "    exp_manager:\n",
      "      explicit_log_dir: /workspace/mount/results/sst2\n",
      "      exp_dir: null\n",
      "      name: trained-model\n",
      "      version: null\n",
      "      use_datetime_version: true\n",
      "      resume_if_exists: true\n",
      "      resume_past_end: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_tensorboard_logger: false\n",
      "      summary_writer_kwargs: null\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs: null\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        filepath: null\n",
      "        monitor: val_loss\n",
      "        verbose: true\n",
      "        save_last: true\n",
      "        save_top_k: 3\n",
      "        save_weights_only: false\n",
      "        mode: auto\n",
      "        period: 1\n",
      "        prefix: null\n",
      "        postfix: .tlt\n",
      "        save_best_model: false\n",
      "      files_to_copy: null\n",
      "    model:\n",
      "      tokenizer:\n",
      "        tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "        vocab_file: null\n",
      "        tokenizer_model: null\n",
      "        special_tokens: null\n",
      "      language_model:\n",
      "        pretrained_model_name: bert-base-uncased\n",
      "        lm_checkpoint: null\n",
      "        config_file: null\n",
      "        config: null\n",
      "      classifier_head:\n",
      "        num_output_layers: 2\n",
      "        fc_dropout: 0.1\n",
      "      class_labels:\n",
      "        class_labels_file: /workspace/mount/data/SST-2/label_ids.csv\n",
      "      dataset:\n",
      "        num_classes: 2\n",
      "        do_lower_case: false\n",
      "        max_seq_length: 256\n",
      "        class_balancing: null\n",
      "        use_cache: false\n",
      "    trainer:\n",
      "      logger: false\n",
      "      checkpoint_callback: false\n",
      "      callbacks: null\n",
      "      default_root_dir: null\n",
      "      gradient_clip_val: 0.0\n",
      "      process_position: 0\n",
      "      num_nodes: 1\n",
      "      num_processes: 1\n",
      "      gpus: 1\n",
      "      auto_select_gpus: false\n",
      "      tpu_cores: null\n",
      "      log_gpu_memory: null\n",
      "      progress_bar_refresh_rate: 1\n",
      "      overfit_batches: 0.0\n",
      "      track_grad_norm: -1\n",
      "      check_val_every_n_epoch: 1\n",
      "      fast_dev_run: false\n",
      "      accumulate_grad_batches: 1\n",
      "      max_epochs: 2\n",
      "      min_epochs: 1\n",
      "      max_steps: null\n",
      "      min_steps: null\n",
      "      limit_train_batches: 1.0\n",
      "      limit_val_batches: 1.0\n",
      "      limit_test_batches: 1.0\n",
      "      val_check_interval: 1.0\n",
      "      flush_logs_every_n_steps: 100\n",
      "      log_every_n_steps: 50\n",
      "      accelerator: ddp\n",
      "      sync_batchnorm: false\n",
      "      precision: 16\n",
      "      weights_summary: full\n",
      "      weights_save_path: null\n",
      "      num_sanity_val_steps: 2\n",
      "      truncated_bptt_steps: null\n",
      "      resume_from_checkpoint: null\n",
      "      profiler: null\n",
      "      benchmark: false\n",
      "      deterministic: false\n",
      "      reload_dataloaders_every_epoch: false\n",
      "      auto_lr_find: false\n",
      "      replace_sampler_ddp: true\n",
      "      terminate_on_nan: false\n",
      "      auto_scale_batch_size: false\n",
      "      prepare_data_per_node: true\n",
      "      amp_backend: native\n",
      "      amp_level: O1\n",
      "    training_ds:\n",
      "      file_path: /workspace/mount/data/SST-2/train.tsv\n",
      "      batch_size: 64\n",
      "      shuffle: true\n",
      "      num_samples: -1\n",
      "      num_workers: 3\n",
      "      drop_last: false\n",
      "      pin_memory: false\n",
      "    validation_ds:\n",
      "      file_path: /workspace/mount/data/SST-2/dev.tsv\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_samples: -1\n",
      "      num_workers: 3\n",
      "      drop_last: false\n",
      "      pin_memory: false\n",
      "    optim:\n",
      "      name: adam\n",
      "      lr: 2.0e-05\n",
      "      betas:\n",
      "      - 0.9\n",
      "      - 0.999\n",
      "      weight_decay: 0.01\n",
      "      sched:\n",
      "        name: WarmupAnnealing\n",
      "        warmup_steps: null\n",
      "        warmup_ratio: 0.1\n",
      "        last_epoch: -1\n",
      "        monitor: val_loss\n",
      "        reduce_on_plateau: false\n",
      "    encryption_key: '********'\n",
      "    \n",
      "GPU available: True, used: True\n",
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n",
      "Using native 16bit precision.\n",
      "[NeMo W 2022-03-05 05:43:23 exp_manager:380] Exp_manager is logging to /workspace/mount/results/sst2, but it already exists.\n",
      "[NeMo W 2022-03-05 05:43:23 exp_manager:303] There was no checkpoint folder at checkpoint_dir :/workspace/mount/results/sst2/checkpoints. Training from scratch.\n",
      "[NeMo I 2022-03-05 05:43:23 exp_manager:194] Experiments will be logged at /workspace/mount/results/sst2\n",
      "Lock 140316008568672 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Downloading: 100% 570/570 [00:00<00:00, 750kB/s]\n",
      "Lock 140316008568672 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Lock 140316008145008 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Downloading: 100% 232k/232k [00:00<00:00, 69.5MB/s]\n",
      "Lock 140316008145008 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Lock 140316008096096 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Downloading: 100% 28.0/28.0 [00:00<00:00, 42.4kB/s]\n",
      "Lock 140316008096096 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Lock 140316008117008 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Downloading: 100% 466k/466k [00:00<00:00, 81.4MB/s]\n",
      "Lock 140316008117008 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "Lock 140316007669088 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Downloading: 100% 440M/440M [00:05<00:00, 86.3MB/s] \n",
      "Lock 140316007669088 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "[NeMo I 2022-03-05 05:43:33 text_classification_dataset:120] Read 67349 examples from /workspace/mount/data/SST-2/train.tsv.\n",
      "[NeMo I 2022-03-05 05:43:33 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2022-03-05 05:43:33 text_classification_dataset:239] example 0: ['from', 'david', \"'s\", 'point', 'of', 'view']\n",
      "[NeMo I 2022-03-05 05:43:33 text_classification_dataset:240] subtokens: [CLS] from david ' s point of view [SEP]\n",
      "[NeMo I 2022-03-05 05:43:33 text_classification_dataset:241] input_ids: 101 2013 2585 1005 1055 2391 1997 3193 102\n",
      "[NeMo I 2022-03-05 05:43:33 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-03-05 05:43:33 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2022-03-05 05:43:33 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2022-03-05 05:43:33 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2022-03-05 05:43:33 text_classification_dataset:239] example 1: ['think', 'of', 'a', 'film', 'more', 'cloyingly']\n",
      "[NeMo I 2022-03-05 05:43:33 text_classification_dataset:240] subtokens: [CLS] think of a film more cl ##oy ##ingly [SEP]\n",
      "[NeMo I 2022-03-05 05:43:33 text_classification_dataset:241] input_ids: 101 2228 1997 1037 2143 2062 18856 6977 15787 102\n",
      "[NeMo I 2022-03-05 05:43:33 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-03-05 05:43:33 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2022-03-05 05:43:33 text_classification_dataset:244] label: 1\n",
      "[NeMo I 2022-03-05 05:44:12 data_preprocessing:295] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2022-03-05 05:44:12 data_preprocessing:297] Min: 3 |                  Max: 66 |                  Mean: 13.319262349849293 |                  Median: 10.0\n",
      "[NeMo I 2022-03-05 05:44:12 data_preprocessing:303] 75 percentile: 18.00\n",
      "[NeMo I 2022-03-05 05:44:12 data_preprocessing:304] 99 percentile: 44.00\n",
      "[NeMo I 2022-03-05 05:44:15 text_classification_dataset:120] Read 872 examples from /workspace/mount/data/SST-2/dev.tsv.\n",
      "[NeMo I 2022-03-05 05:44:15 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2022-03-05 05:44:15 text_classification_dataset:239] example 0: ['it', \"'s\", 'a', 'charming', 'and', 'often', 'affecting', 'journey', '.']\n",
      "[NeMo I 2022-03-05 05:44:15 text_classification_dataset:240] subtokens: [CLS] it ' s a charming and often affecting journey . [SEP]\n",
      "[NeMo I 2022-03-05 05:44:15 text_classification_dataset:241] input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102\n",
      "[NeMo I 2022-03-05 05:44:15 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-03-05 05:44:15 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2022-03-05 05:44:15 text_classification_dataset:244] label: 1\n",
      "[NeMo I 2022-03-05 05:44:15 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2022-03-05 05:44:15 text_classification_dataset:239] example 1: ['unflinchingly', 'bleak', 'and', 'desperate']\n",
      "[NeMo I 2022-03-05 05:44:15 text_classification_dataset:240] subtokens: [CLS] un ##fl ##in ##ching ##ly bleak and desperate [SEP]\n",
      "[NeMo I 2022-03-05 05:44:15 text_classification_dataset:241] input_ids: 101 4895 10258 2378 8450 2135 21657 1998 7143 102\n",
      "[NeMo I 2022-03-05 05:44:15 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-03-05 05:44:15 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2022-03-05 05:44:15 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2022-03-05 05:44:16 data_preprocessing:295] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2022-03-05 05:44:16 data_preprocessing:297] Min: 4 |                  Max: 55 |                  Mean: 25.163990825688074 |                  Median: 24.5\n",
      "[NeMo I 2022-03-05 05:44:16 data_preprocessing:303] 75 percentile: 32.00\n",
      "[NeMo I 2022-03-05 05:44:16 data_preprocessing:304] 99 percentile: 51.29\n",
      "[NeMo I 2022-03-05 05:44:16 modelPT:753] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.999]\n",
      "        eps: 1e-08\n",
      "        lr: 2e-05\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2022-03-05 05:44:16 lr_scheduler:617] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7f9dc4889f40>\" \n",
      "    will be used during training (effective maximum steps = 2106) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 2106\n",
      "    )\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "[NeMo I 2022-03-05 05:44:17 modelPT:627] No optimizer config provided, therefore no optimizer was created\n",
      "\n",
      "    | Name                                                   | Type                 | Params\n",
      "--------------------------------------------------------------------------------------------------\n",
      "0   | bert_model                                             | BertEncoder          | 109 M \n",
      "1   | bert_model.embeddings                                  | BertEmbeddings       | 23.8 M\n",
      "2   | bert_model.embeddings.word_embeddings                  | Embedding            | 23.4 M\n",
      "3   | bert_model.embeddings.position_embeddings              | Embedding            | 393 K \n",
      "4   | bert_model.embeddings.token_type_embeddings            | Embedding            | 1.5 K \n",
      "5   | bert_model.embeddings.LayerNorm                        | LayerNorm            | 1.5 K \n",
      "6   | bert_model.embeddings.dropout                          | Dropout              | 0     \n",
      "7   | bert_model.encoder                                     | BertEncoder          | 85.1 M\n",
      "8   | bert_model.encoder.layer                               | ModuleList           | 85.1 M\n",
      "9   | bert_model.encoder.layer.0                             | BertLayer            | 7.1 M \n",
      "10  | bert_model.encoder.layer.0.attention                   | BertAttention        | 2.4 M \n",
      "11  | bert_model.encoder.layer.0.attention.self              | BertSelfAttention    | 1.8 M \n",
      "12  | bert_model.encoder.layer.0.attention.self.query        | Linear               | 590 K \n",
      "13  | bert_model.encoder.layer.0.attention.self.key          | Linear               | 590 K \n",
      "14  | bert_model.encoder.layer.0.attention.self.value        | Linear               | 590 K \n",
      "15  | bert_model.encoder.layer.0.attention.self.dropout      | Dropout              | 0     \n",
      "16  | bert_model.encoder.layer.0.attention.output            | BertSelfOutput       | 592 K \n",
      "17  | bert_model.encoder.layer.0.attention.output.dense      | Linear               | 590 K \n",
      "18  | bert_model.encoder.layer.0.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "19  | bert_model.encoder.layer.0.attention.output.dropout    | Dropout              | 0     \n",
      "20  | bert_model.encoder.layer.0.intermediate                | BertIntermediate     | 2.4 M \n",
      "21  | bert_model.encoder.layer.0.intermediate.dense          | Linear               | 2.4 M \n",
      "22  | bert_model.encoder.layer.0.output                      | BertOutput           | 2.4 M \n",
      "23  | bert_model.encoder.layer.0.output.dense                | Linear               | 2.4 M \n",
      "24  | bert_model.encoder.layer.0.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "25  | bert_model.encoder.layer.0.output.dropout              | Dropout              | 0     \n",
      "26  | bert_model.encoder.layer.1                             | BertLayer            | 7.1 M \n",
      "27  | bert_model.encoder.layer.1.attention                   | BertAttention        | 2.4 M \n",
      "28  | bert_model.encoder.layer.1.attention.self              | BertSelfAttention    | 1.8 M \n",
      "29  | bert_model.encoder.layer.1.attention.self.query        | Linear               | 590 K \n",
      "30  | bert_model.encoder.layer.1.attention.self.key          | Linear               | 590 K \n",
      "31  | bert_model.encoder.layer.1.attention.self.value        | Linear               | 590 K \n",
      "32  | bert_model.encoder.layer.1.attention.self.dropout      | Dropout              | 0     \n",
      "33  | bert_model.encoder.layer.1.attention.output            | BertSelfOutput       | 592 K \n",
      "34  | bert_model.encoder.layer.1.attention.output.dense      | Linear               | 590 K \n",
      "35  | bert_model.encoder.layer.1.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "36  | bert_model.encoder.layer.1.attention.output.dropout    | Dropout              | 0     \n",
      "37  | bert_model.encoder.layer.1.intermediate                | BertIntermediate     | 2.4 M \n",
      "38  | bert_model.encoder.layer.1.intermediate.dense          | Linear               | 2.4 M \n",
      "39  | bert_model.encoder.layer.1.output                      | BertOutput           | 2.4 M \n",
      "40  | bert_model.encoder.layer.1.output.dense                | Linear               | 2.4 M \n",
      "41  | bert_model.encoder.layer.1.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "42  | bert_model.encoder.layer.1.output.dropout              | Dropout              | 0     \n",
      "43  | bert_model.encoder.layer.2                             | BertLayer            | 7.1 M \n",
      "44  | bert_model.encoder.layer.2.attention                   | BertAttention        | 2.4 M \n",
      "45  | bert_model.encoder.layer.2.attention.self              | BertSelfAttention    | 1.8 M \n",
      "46  | bert_model.encoder.layer.2.attention.self.query        | Linear               | 590 K \n",
      "47  | bert_model.encoder.layer.2.attention.self.key          | Linear               | 590 K \n",
      "48  | bert_model.encoder.layer.2.attention.self.value        | Linear               | 590 K \n",
      "49  | bert_model.encoder.layer.2.attention.self.dropout      | Dropout              | 0     \n",
      "50  | bert_model.encoder.layer.2.attention.output            | BertSelfOutput       | 592 K \n",
      "51  | bert_model.encoder.layer.2.attention.output.dense      | Linear               | 590 K \n",
      "52  | bert_model.encoder.layer.2.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "53  | bert_model.encoder.layer.2.attention.output.dropout    | Dropout              | 0     \n",
      "54  | bert_model.encoder.layer.2.intermediate                | BertIntermediate     | 2.4 M \n",
      "55  | bert_model.encoder.layer.2.intermediate.dense          | Linear               | 2.4 M \n",
      "56  | bert_model.encoder.layer.2.output                      | BertOutput           | 2.4 M \n",
      "57  | bert_model.encoder.layer.2.output.dense                | Linear               | 2.4 M \n",
      "58  | bert_model.encoder.layer.2.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "59  | bert_model.encoder.layer.2.output.dropout              | Dropout              | 0     \n",
      "60  | bert_model.encoder.layer.3                             | BertLayer            | 7.1 M \n",
      "61  | bert_model.encoder.layer.3.attention                   | BertAttention        | 2.4 M \n",
      "62  | bert_model.encoder.layer.3.attention.self              | BertSelfAttention    | 1.8 M \n",
      "63  | bert_model.encoder.layer.3.attention.self.query        | Linear               | 590 K \n",
      "64  | bert_model.encoder.layer.3.attention.self.key          | Linear               | 590 K \n",
      "65  | bert_model.encoder.layer.3.attention.self.value        | Linear               | 590 K \n",
      "66  | bert_model.encoder.layer.3.attention.self.dropout      | Dropout              | 0     \n",
      "67  | bert_model.encoder.layer.3.attention.output            | BertSelfOutput       | 592 K \n",
      "68  | bert_model.encoder.layer.3.attention.output.dense      | Linear               | 590 K \n",
      "69  | bert_model.encoder.layer.3.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "70  | bert_model.encoder.layer.3.attention.output.dropout    | Dropout              | 0     \n",
      "71  | bert_model.encoder.layer.3.intermediate                | BertIntermediate     | 2.4 M \n",
      "72  | bert_model.encoder.layer.3.intermediate.dense          | Linear               | 2.4 M \n",
      "73  | bert_model.encoder.layer.3.output                      | BertOutput           | 2.4 M \n",
      "74  | bert_model.encoder.layer.3.output.dense                | Linear               | 2.4 M \n",
      "75  | bert_model.encoder.layer.3.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "76  | bert_model.encoder.layer.3.output.dropout              | Dropout              | 0     \n",
      "77  | bert_model.encoder.layer.4                             | BertLayer            | 7.1 M \n",
      "78  | bert_model.encoder.layer.4.attention                   | BertAttention        | 2.4 M \n",
      "79  | bert_model.encoder.layer.4.attention.self              | BertSelfAttention    | 1.8 M \n",
      "80  | bert_model.encoder.layer.4.attention.self.query        | Linear               | 590 K \n",
      "81  | bert_model.encoder.layer.4.attention.self.key          | Linear               | 590 K \n",
      "82  | bert_model.encoder.layer.4.attention.self.value        | Linear               | 590 K \n",
      "83  | bert_model.encoder.layer.4.attention.self.dropout      | Dropout              | 0     \n",
      "84  | bert_model.encoder.layer.4.attention.output            | BertSelfOutput       | 592 K \n",
      "85  | bert_model.encoder.layer.4.attention.output.dense      | Linear               | 590 K \n",
      "86  | bert_model.encoder.layer.4.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "87  | bert_model.encoder.layer.4.attention.output.dropout    | Dropout              | 0     \n",
      "88  | bert_model.encoder.layer.4.intermediate                | BertIntermediate     | 2.4 M \n",
      "89  | bert_model.encoder.layer.4.intermediate.dense          | Linear               | 2.4 M \n",
      "90  | bert_model.encoder.layer.4.output                      | BertOutput           | 2.4 M \n",
      "91  | bert_model.encoder.layer.4.output.dense                | Linear               | 2.4 M \n",
      "92  | bert_model.encoder.layer.4.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "93  | bert_model.encoder.layer.4.output.dropout              | Dropout              | 0     \n",
      "94  | bert_model.encoder.layer.5                             | BertLayer            | 7.1 M \n",
      "95  | bert_model.encoder.layer.5.attention                   | BertAttention        | 2.4 M \n",
      "96  | bert_model.encoder.layer.5.attention.self              | BertSelfAttention    | 1.8 M \n",
      "97  | bert_model.encoder.layer.5.attention.self.query        | Linear               | 590 K \n",
      "98  | bert_model.encoder.layer.5.attention.self.key          | Linear               | 590 K \n",
      "99  | bert_model.encoder.layer.5.attention.self.value        | Linear               | 590 K \n",
      "100 | bert_model.encoder.layer.5.attention.self.dropout      | Dropout              | 0     \n",
      "101 | bert_model.encoder.layer.5.attention.output            | BertSelfOutput       | 592 K \n",
      "102 | bert_model.encoder.layer.5.attention.output.dense      | Linear               | 590 K \n",
      "103 | bert_model.encoder.layer.5.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "104 | bert_model.encoder.layer.5.attention.output.dropout    | Dropout              | 0     \n",
      "105 | bert_model.encoder.layer.5.intermediate                | BertIntermediate     | 2.4 M \n",
      "106 | bert_model.encoder.layer.5.intermediate.dense          | Linear               | 2.4 M \n",
      "107 | bert_model.encoder.layer.5.output                      | BertOutput           | 2.4 M \n",
      "108 | bert_model.encoder.layer.5.output.dense                | Linear               | 2.4 M \n",
      "109 | bert_model.encoder.layer.5.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "110 | bert_model.encoder.layer.5.output.dropout              | Dropout              | 0     \n",
      "111 | bert_model.encoder.layer.6                             | BertLayer            | 7.1 M \n",
      "112 | bert_model.encoder.layer.6.attention                   | BertAttention        | 2.4 M \n",
      "113 | bert_model.encoder.layer.6.attention.self              | BertSelfAttention    | 1.8 M \n",
      "114 | bert_model.encoder.layer.6.attention.self.query        | Linear               | 590 K \n",
      "115 | bert_model.encoder.layer.6.attention.self.key          | Linear               | 590 K \n",
      "116 | bert_model.encoder.layer.6.attention.self.value        | Linear               | 590 K \n",
      "117 | bert_model.encoder.layer.6.attention.self.dropout      | Dropout              | 0     \n",
      "118 | bert_model.encoder.layer.6.attention.output            | BertSelfOutput       | 592 K \n",
      "119 | bert_model.encoder.layer.6.attention.output.dense      | Linear               | 590 K \n",
      "120 | bert_model.encoder.layer.6.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "121 | bert_model.encoder.layer.6.attention.output.dropout    | Dropout              | 0     \n",
      "122 | bert_model.encoder.layer.6.intermediate                | BertIntermediate     | 2.4 M \n",
      "123 | bert_model.encoder.layer.6.intermediate.dense          | Linear               | 2.4 M \n",
      "124 | bert_model.encoder.layer.6.output                      | BertOutput           | 2.4 M \n",
      "125 | bert_model.encoder.layer.6.output.dense                | Linear               | 2.4 M \n",
      "126 | bert_model.encoder.layer.6.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "127 | bert_model.encoder.layer.6.output.dropout              | Dropout              | 0     \n",
      "128 | bert_model.encoder.layer.7                             | BertLayer            | 7.1 M \n",
      "129 | bert_model.encoder.layer.7.attention                   | BertAttention        | 2.4 M \n",
      "130 | bert_model.encoder.layer.7.attention.self              | BertSelfAttention    | 1.8 M \n",
      "131 | bert_model.encoder.layer.7.attention.self.query        | Linear               | 590 K \n",
      "132 | bert_model.encoder.layer.7.attention.self.key          | Linear               | 590 K \n",
      "133 | bert_model.encoder.layer.7.attention.self.value        | Linear               | 590 K \n",
      "134 | bert_model.encoder.layer.7.attention.self.dropout      | Dropout              | 0     \n",
      "135 | bert_model.encoder.layer.7.attention.output            | BertSelfOutput       | 592 K \n",
      "136 | bert_model.encoder.layer.7.attention.output.dense      | Linear               | 590 K \n",
      "137 | bert_model.encoder.layer.7.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "138 | bert_model.encoder.layer.7.attention.output.dropout    | Dropout              | 0     \n",
      "139 | bert_model.encoder.layer.7.intermediate                | BertIntermediate     | 2.4 M \n",
      "140 | bert_model.encoder.layer.7.intermediate.dense          | Linear               | 2.4 M \n",
      "141 | bert_model.encoder.layer.7.output                      | BertOutput           | 2.4 M \n",
      "142 | bert_model.encoder.layer.7.output.dense                | Linear               | 2.4 M \n",
      "143 | bert_model.encoder.layer.7.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "144 | bert_model.encoder.layer.7.output.dropout              | Dropout              | 0     \n",
      "145 | bert_model.encoder.layer.8                             | BertLayer            | 7.1 M \n",
      "146 | bert_model.encoder.layer.8.attention                   | BertAttention        | 2.4 M \n",
      "147 | bert_model.encoder.layer.8.attention.self              | BertSelfAttention    | 1.8 M \n",
      "148 | bert_model.encoder.layer.8.attention.self.query        | Linear               | 590 K \n",
      "149 | bert_model.encoder.layer.8.attention.self.key          | Linear               | 590 K \n",
      "150 | bert_model.encoder.layer.8.attention.self.value        | Linear               | 590 K \n",
      "151 | bert_model.encoder.layer.8.attention.self.dropout      | Dropout              | 0     \n",
      "152 | bert_model.encoder.layer.8.attention.output            | BertSelfOutput       | 592 K \n",
      "153 | bert_model.encoder.layer.8.attention.output.dense      | Linear               | 590 K \n",
      "154 | bert_model.encoder.layer.8.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "155 | bert_model.encoder.layer.8.attention.output.dropout    | Dropout              | 0     \n",
      "156 | bert_model.encoder.layer.8.intermediate                | BertIntermediate     | 2.4 M \n",
      "157 | bert_model.encoder.layer.8.intermediate.dense          | Linear               | 2.4 M \n",
      "158 | bert_model.encoder.layer.8.output                      | BertOutput           | 2.4 M \n",
      "159 | bert_model.encoder.layer.8.output.dense                | Linear               | 2.4 M \n",
      "160 | bert_model.encoder.layer.8.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "161 | bert_model.encoder.layer.8.output.dropout              | Dropout              | 0     \n",
      "162 | bert_model.encoder.layer.9                             | BertLayer            | 7.1 M \n",
      "163 | bert_model.encoder.layer.9.attention                   | BertAttention        | 2.4 M \n",
      "164 | bert_model.encoder.layer.9.attention.self              | BertSelfAttention    | 1.8 M \n",
      "165 | bert_model.encoder.layer.9.attention.self.query        | Linear               | 590 K \n",
      "166 | bert_model.encoder.layer.9.attention.self.key          | Linear               | 590 K \n",
      "167 | bert_model.encoder.layer.9.attention.self.value        | Linear               | 590 K \n",
      "168 | bert_model.encoder.layer.9.attention.self.dropout      | Dropout              | 0     \n",
      "169 | bert_model.encoder.layer.9.attention.output            | BertSelfOutput       | 592 K \n",
      "170 | bert_model.encoder.layer.9.attention.output.dense      | Linear               | 590 K \n",
      "171 | bert_model.encoder.layer.9.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "172 | bert_model.encoder.layer.9.attention.output.dropout    | Dropout              | 0     \n",
      "173 | bert_model.encoder.layer.9.intermediate                | BertIntermediate     | 2.4 M \n",
      "174 | bert_model.encoder.layer.9.intermediate.dense          | Linear               | 2.4 M \n",
      "175 | bert_model.encoder.layer.9.output                      | BertOutput           | 2.4 M \n",
      "176 | bert_model.encoder.layer.9.output.dense                | Linear               | 2.4 M \n",
      "177 | bert_model.encoder.layer.9.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "178 | bert_model.encoder.layer.9.output.dropout              | Dropout              | 0     \n",
      "179 | bert_model.encoder.layer.10                            | BertLayer            | 7.1 M \n",
      "180 | bert_model.encoder.layer.10.attention                  | BertAttention        | 2.4 M \n",
      "181 | bert_model.encoder.layer.10.attention.self             | BertSelfAttention    | 1.8 M \n",
      "182 | bert_model.encoder.layer.10.attention.self.query       | Linear               | 590 K \n",
      "183 | bert_model.encoder.layer.10.attention.self.key         | Linear               | 590 K \n",
      "184 | bert_model.encoder.layer.10.attention.self.value       | Linear               | 590 K \n",
      "185 | bert_model.encoder.layer.10.attention.self.dropout     | Dropout              | 0     \n",
      "186 | bert_model.encoder.layer.10.attention.output           | BertSelfOutput       | 592 K \n",
      "187 | bert_model.encoder.layer.10.attention.output.dense     | Linear               | 590 K \n",
      "188 | bert_model.encoder.layer.10.attention.output.LayerNorm | LayerNorm            | 1.5 K \n",
      "189 | bert_model.encoder.layer.10.attention.output.dropout   | Dropout              | 0     \n",
      "190 | bert_model.encoder.layer.10.intermediate               | BertIntermediate     | 2.4 M \n",
      "191 | bert_model.encoder.layer.10.intermediate.dense         | Linear               | 2.4 M \n",
      "192 | bert_model.encoder.layer.10.output                     | BertOutput           | 2.4 M \n",
      "193 | bert_model.encoder.layer.10.output.dense               | Linear               | 2.4 M \n",
      "194 | bert_model.encoder.layer.10.output.LayerNorm           | LayerNorm            | 1.5 K \n",
      "195 | bert_model.encoder.layer.10.output.dropout             | Dropout              | 0     \n",
      "196 | bert_model.encoder.layer.11                            | BertLayer            | 7.1 M \n",
      "197 | bert_model.encoder.layer.11.attention                  | BertAttention        | 2.4 M \n",
      "198 | bert_model.encoder.layer.11.attention.self             | BertSelfAttention    | 1.8 M \n",
      "199 | bert_model.encoder.layer.11.attention.self.query       | Linear               | 590 K \n",
      "200 | bert_model.encoder.layer.11.attention.self.key         | Linear               | 590 K \n",
      "201 | bert_model.encoder.layer.11.attention.self.value       | Linear               | 590 K \n",
      "202 | bert_model.encoder.layer.11.attention.self.dropout     | Dropout              | 0     \n",
      "203 | bert_model.encoder.layer.11.attention.output           | BertSelfOutput       | 592 K \n",
      "204 | bert_model.encoder.layer.11.attention.output.dense     | Linear               | 590 K \n",
      "205 | bert_model.encoder.layer.11.attention.output.LayerNorm | LayerNorm            | 1.5 K \n",
      "206 | bert_model.encoder.layer.11.attention.output.dropout   | Dropout              | 0     \n",
      "207 | bert_model.encoder.layer.11.intermediate               | BertIntermediate     | 2.4 M \n",
      "208 | bert_model.encoder.layer.11.intermediate.dense         | Linear               | 2.4 M \n",
      "209 | bert_model.encoder.layer.11.output                     | BertOutput           | 2.4 M \n",
      "210 | bert_model.encoder.layer.11.output.dense               | Linear               | 2.4 M \n",
      "211 | bert_model.encoder.layer.11.output.LayerNorm           | LayerNorm            | 1.5 K \n",
      "212 | bert_model.encoder.layer.11.output.dropout             | Dropout              | 0     \n",
      "213 | bert_model.pooler                                      | BertPooler           | 590 K \n",
      "214 | bert_model.pooler.dense                                | Linear               | 590 K \n",
      "215 | bert_model.pooler.activation                           | Tanh                 | 0     \n",
      "216 | classifier                                             | SequenceClassifier   | 592 K \n",
      "217 | classifier.dropout                                     | Dropout              | 0     \n",
      "218 | classifier.mlp                                         | MultiLayerPerceptron | 592 K \n",
      "219 | classifier.mlp.layer0                                  | Linear               | 590 K \n",
      "220 | classifier.mlp.layer2                                  | Linear               | 1.5 K \n",
      "221 | loss                                                   | CrossEntropyLoss     | 0     \n",
      "222 | classification_report                                  | ClassificationReport | 0     \n",
      "--------------------------------------------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "\n",
      "    | Name                                                   | Type                 | Params\n",
      "--------------------------------------------------------------------------------------------------\n",
      "0   | bert_model                                             | BertEncoder          | 109 M \n",
      "1   | bert_model.embeddings                                  | BertEmbeddings       | 23.8 M\n",
      "2   | bert_model.embeddings.word_embeddings                  | Embedding            | 23.4 M\n",
      "3   | bert_model.embeddings.position_embeddings              | Embedding            | 393 K \n",
      "4   | bert_model.embeddings.token_type_embeddings            | Embedding            | 1.5 K \n",
      "5   | bert_model.embeddings.LayerNorm                        | LayerNorm            | 1.5 K \n",
      "6   | bert_model.embeddings.dropout                          | Dropout              | 0     \n",
      "7   | bert_model.encoder                                     | BertEncoder          | 85.1 M\n",
      "8   | bert_model.encoder.layer                               | ModuleList           | 85.1 M\n",
      "9   | bert_model.encoder.layer.0                             | BertLayer            | 7.1 M \n",
      "10  | bert_model.encoder.layer.0.attention                   | BertAttention        | 2.4 M \n",
      "11  | bert_model.encoder.layer.0.attention.self              | BertSelfAttention    | 1.8 M \n",
      "12  | bert_model.encoder.layer.0.attention.self.query        | Linear               | 590 K \n",
      "13  | bert_model.encoder.layer.0.attention.self.key          | Linear               | 590 K \n",
      "14  | bert_model.encoder.layer.0.attention.self.value        | Linear               | 590 K \n",
      "15  | bert_model.encoder.layer.0.attention.self.dropout      | Dropout              | 0     \n",
      "16  | bert_model.encoder.layer.0.attention.output            | BertSelfOutput       | 592 K \n",
      "17  | bert_model.encoder.layer.0.attention.output.dense      | Linear               | 590 K \n",
      "18  | bert_model.encoder.layer.0.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "19  | bert_model.encoder.layer.0.attention.output.dropout    | Dropout              | 0     \n",
      "20  | bert_model.encoder.layer.0.intermediate                | BertIntermediate     | 2.4 M \n",
      "21  | bert_model.encoder.layer.0.intermediate.dense          | Linear               | 2.4 M \n",
      "22  | bert_model.encoder.layer.0.output                      | BertOutput           | 2.4 M \n",
      "23  | bert_model.encoder.layer.0.output.dense                | Linear               | 2.4 M \n",
      "24  | bert_model.encoder.layer.0.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "25  | bert_model.encoder.layer.0.output.dropout              | Dropout              | 0     \n",
      "26  | bert_model.encoder.layer.1                             | BertLayer            | 7.1 M \n",
      "27  | bert_model.encoder.layer.1.attention                   | BertAttention        | 2.4 M \n",
      "28  | bert_model.encoder.layer.1.attention.self              | BertSelfAttention    | 1.8 M \n",
      "29  | bert_model.encoder.layer.1.attention.self.query        | Linear               | 590 K \n",
      "30  | bert_model.encoder.layer.1.attention.self.key          | Linear               | 590 K \n",
      "31  | bert_model.encoder.layer.1.attention.self.value        | Linear               | 590 K \n",
      "32  | bert_model.encoder.layer.1.attention.self.dropout      | Dropout              | 0     \n",
      "33  | bert_model.encoder.layer.1.attention.output            | BertSelfOutput       | 592 K \n",
      "34  | bert_model.encoder.layer.1.attention.output.dense      | Linear               | 590 K \n",
      "35  | bert_model.encoder.layer.1.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "36  | bert_model.encoder.layer.1.attention.output.dropout    | Dropout              | 0     \n",
      "37  | bert_model.encoder.layer.1.intermediate                | BertIntermediate     | 2.4 M \n",
      "38  | bert_model.encoder.layer.1.intermediate.dense          | Linear               | 2.4 M \n",
      "39  | bert_model.encoder.layer.1.output                      | BertOutput           | 2.4 M \n",
      "40  | bert_model.encoder.layer.1.output.dense                | Linear               | 2.4 M \n",
      "41  | bert_model.encoder.layer.1.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "42  | bert_model.encoder.layer.1.output.dropout              | Dropout              | 0     \n",
      "43  | bert_model.encoder.layer.2                             | BertLayer            | 7.1 M \n",
      "44  | bert_model.encoder.layer.2.attention                   | BertAttention        | 2.4 M \n",
      "45  | bert_model.encoder.layer.2.attention.self              | BertSelfAttention    | 1.8 M \n",
      "46  | bert_model.encoder.layer.2.attention.self.query        | Linear               | 590 K \n",
      "47  | bert_model.encoder.layer.2.attention.self.key          | Linear               | 590 K \n",
      "48  | bert_model.encoder.layer.2.attention.self.value        | Linear               | 590 K \n",
      "49  | bert_model.encoder.layer.2.attention.self.dropout      | Dropout              | 0     \n",
      "50  | bert_model.encoder.layer.2.attention.output            | BertSelfOutput       | 592 K \n",
      "51  | bert_model.encoder.layer.2.attention.output.dense      | Linear               | 590 K \n",
      "52  | bert_model.encoder.layer.2.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "53  | bert_model.encoder.layer.2.attention.output.dropout    | Dropout              | 0     \n",
      "54  | bert_model.encoder.layer.2.intermediate                | BertIntermediate     | 2.4 M \n",
      "55  | bert_model.encoder.layer.2.intermediate.dense          | Linear               | 2.4 M \n",
      "56  | bert_model.encoder.layer.2.output                      | BertOutput           | 2.4 M \n",
      "57  | bert_model.encoder.layer.2.output.dense                | Linear               | 2.4 M \n",
      "58  | bert_model.encoder.layer.2.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "59  | bert_model.encoder.layer.2.output.dropout              | Dropout              | 0     \n",
      "60  | bert_model.encoder.layer.3                             | BertLayer            | 7.1 M \n",
      "61  | bert_model.encoder.layer.3.attention                   | BertAttention        | 2.4 M \n",
      "62  | bert_model.encoder.layer.3.attention.self              | BertSelfAttention    | 1.8 M \n",
      "63  | bert_model.encoder.layer.3.attention.self.query        | Linear               | 590 K \n",
      "64  | bert_model.encoder.layer.3.attention.self.key          | Linear               | 590 K \n",
      "65  | bert_model.encoder.layer.3.attention.self.value        | Linear               | 590 K \n",
      "66  | bert_model.encoder.layer.3.attention.self.dropout      | Dropout              | 0     \n",
      "67  | bert_model.encoder.layer.3.attention.output            | BertSelfOutput       | 592 K \n",
      "68  | bert_model.encoder.layer.3.attention.output.dense      | Linear               | 590 K \n",
      "69  | bert_model.encoder.layer.3.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "70  | bert_model.encoder.layer.3.attention.output.dropout    | Dropout              | 0     \n",
      "71  | bert_model.encoder.layer.3.intermediate                | BertIntermediate     | 2.4 M \n",
      "72  | bert_model.encoder.layer.3.intermediate.dense          | Linear               | 2.4 M \n",
      "73  | bert_model.encoder.layer.3.output                      | BertOutput           | 2.4 M \n",
      "74  | bert_model.encoder.layer.3.output.dense                | Linear               | 2.4 M \n",
      "75  | bert_model.encoder.layer.3.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "76  | bert_model.encoder.layer.3.output.dropout              | Dropout              | 0     \n",
      "77  | bert_model.encoder.layer.4                             | BertLayer            | 7.1 M \n",
      "78  | bert_model.encoder.layer.4.attention                   | BertAttention        | 2.4 M \n",
      "79  | bert_model.encoder.layer.4.attention.self              | BertSelfAttention    | 1.8 M \n",
      "80  | bert_model.encoder.layer.4.attention.self.query        | Linear               | 590 K \n",
      "81  | bert_model.encoder.layer.4.attention.self.key          | Linear               | 590 K \n",
      "82  | bert_model.encoder.layer.4.attention.self.value        | Linear               | 590 K \n",
      "83  | bert_model.encoder.layer.4.attention.self.dropout      | Dropout              | 0     \n",
      "84  | bert_model.encoder.layer.4.attention.output            | BertSelfOutput       | 592 K \n",
      "85  | bert_model.encoder.layer.4.attention.output.dense      | Linear               | 590 K \n",
      "86  | bert_model.encoder.layer.4.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "87  | bert_model.encoder.layer.4.attention.output.dropout    | Dropout              | 0     \n",
      "88  | bert_model.encoder.layer.4.intermediate                | BertIntermediate     | 2.4 M \n",
      "89  | bert_model.encoder.layer.4.intermediate.dense          | Linear               | 2.4 M \n",
      "90  | bert_model.encoder.layer.4.output                      | BertOutput           | 2.4 M \n",
      "91  | bert_model.encoder.layer.4.output.dense                | Linear               | 2.4 M \n",
      "92  | bert_model.encoder.layer.4.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "93  | bert_model.encoder.layer.4.output.dropout              | Dropout              | 0     \n",
      "94  | bert_model.encoder.layer.5                             | BertLayer            | 7.1 M \n",
      "95  | bert_model.encoder.layer.5.attention                   | BertAttention        | 2.4 M \n",
      "96  | bert_model.encoder.layer.5.attention.self              | BertSelfAttention    | 1.8 M \n",
      "97  | bert_model.encoder.layer.5.attention.self.query        | Linear               | 590 K \n",
      "98  | bert_model.encoder.layer.5.attention.self.key          | Linear               | 590 K \n",
      "99  | bert_model.encoder.layer.5.attention.self.value        | Linear               | 590 K \n",
      "100 | bert_model.encoder.layer.5.attention.self.dropout      | Dropout              | 0     \n",
      "101 | bert_model.encoder.layer.5.attention.output            | BertSelfOutput       | 592 K \n",
      "102 | bert_model.encoder.layer.5.attention.output.dense      | Linear               | 590 K \n",
      "103 | bert_model.encoder.layer.5.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "104 | bert_model.encoder.layer.5.attention.output.dropout    | Dropout              | 0     \n",
      "105 | bert_model.encoder.layer.5.intermediate                | BertIntermediate     | 2.4 M \n",
      "106 | bert_model.encoder.layer.5.intermediate.dense          | Linear               | 2.4 M \n",
      "107 | bert_model.encoder.layer.5.output                      | BertOutput           | 2.4 M \n",
      "108 | bert_model.encoder.layer.5.output.dense                | Linear               | 2.4 M \n",
      "109 | bert_model.encoder.layer.5.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "110 | bert_model.encoder.layer.5.output.dropout              | Dropout              | 0     \n",
      "111 | bert_model.encoder.layer.6                             | BertLayer            | 7.1 M \n",
      "112 | bert_model.encoder.layer.6.attention                   | BertAttention        | 2.4 M \n",
      "113 | bert_model.encoder.layer.6.attention.self              | BertSelfAttention    | 1.8 M \n",
      "114 | bert_model.encoder.layer.6.attention.self.query        | Linear               | 590 K \n",
      "115 | bert_model.encoder.layer.6.attention.self.key          | Linear               | 590 K \n",
      "116 | bert_model.encoder.layer.6.attention.self.value        | Linear               | 590 K \n",
      "117 | bert_model.encoder.layer.6.attention.self.dropout      | Dropout              | 0     \n",
      "118 | bert_model.encoder.layer.6.attention.output            | BertSelfOutput       | 592 K \n",
      "119 | bert_model.encoder.layer.6.attention.output.dense      | Linear               | 590 K \n",
      "120 | bert_model.encoder.layer.6.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "121 | bert_model.encoder.layer.6.attention.output.dropout    | Dropout              | 0     \n",
      "122 | bert_model.encoder.layer.6.intermediate                | BertIntermediate     | 2.4 M \n",
      "123 | bert_model.encoder.layer.6.intermediate.dense          | Linear               | 2.4 M \n",
      "124 | bert_model.encoder.layer.6.output                      | BertOutput           | 2.4 M \n",
      "125 | bert_model.encoder.layer.6.output.dense                | Linear               | 2.4 M \n",
      "126 | bert_model.encoder.layer.6.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "127 | bert_model.encoder.layer.6.output.dropout              | Dropout              | 0     \n",
      "128 | bert_model.encoder.layer.7                             | BertLayer            | 7.1 M \n",
      "129 | bert_model.encoder.layer.7.attention                   | BertAttention        | 2.4 M \n",
      "130 | bert_model.encoder.layer.7.attention.self              | BertSelfAttention    | 1.8 M \n",
      "131 | bert_model.encoder.layer.7.attention.self.query        | Linear               | 590 K \n",
      "132 | bert_model.encoder.layer.7.attention.self.key          | Linear               | 590 K \n",
      "133 | bert_model.encoder.layer.7.attention.self.value        | Linear               | 590 K \n",
      "134 | bert_model.encoder.layer.7.attention.self.dropout      | Dropout              | 0     \n",
      "135 | bert_model.encoder.layer.7.attention.output            | BertSelfOutput       | 592 K \n",
      "136 | bert_model.encoder.layer.7.attention.output.dense      | Linear               | 590 K \n",
      "137 | bert_model.encoder.layer.7.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "138 | bert_model.encoder.layer.7.attention.output.dropout    | Dropout              | 0     \n",
      "139 | bert_model.encoder.layer.7.intermediate                | BertIntermediate     | 2.4 M \n",
      "140 | bert_model.encoder.layer.7.intermediate.dense          | Linear               | 2.4 M \n",
      "141 | bert_model.encoder.layer.7.output                      | BertOutput           | 2.4 M \n",
      "142 | bert_model.encoder.layer.7.output.dense                | Linear               | 2.4 M \n",
      "143 | bert_model.encoder.layer.7.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "144 | bert_model.encoder.layer.7.output.dropout              | Dropout              | 0     \n",
      "145 | bert_model.encoder.layer.8                             | BertLayer            | 7.1 M \n",
      "146 | bert_model.encoder.layer.8.attention                   | BertAttention        | 2.4 M \n",
      "147 | bert_model.encoder.layer.8.attention.self              | BertSelfAttention    | 1.8 M \n",
      "148 | bert_model.encoder.layer.8.attention.self.query        | Linear               | 590 K \n",
      "149 | bert_model.encoder.layer.8.attention.self.key          | Linear               | 590 K \n",
      "150 | bert_model.encoder.layer.8.attention.self.value        | Linear               | 590 K \n",
      "151 | bert_model.encoder.layer.8.attention.self.dropout      | Dropout              | 0     \n",
      "152 | bert_model.encoder.layer.8.attention.output            | BertSelfOutput       | 592 K \n",
      "153 | bert_model.encoder.layer.8.attention.output.dense      | Linear               | 590 K \n",
      "154 | bert_model.encoder.layer.8.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "155 | bert_model.encoder.layer.8.attention.output.dropout    | Dropout              | 0     \n",
      "156 | bert_model.encoder.layer.8.intermediate                | BertIntermediate     | 2.4 M \n",
      "157 | bert_model.encoder.layer.8.intermediate.dense          | Linear               | 2.4 M \n",
      "158 | bert_model.encoder.layer.8.output                      | BertOutput           | 2.4 M \n",
      "159 | bert_model.encoder.layer.8.output.dense                | Linear               | 2.4 M \n",
      "160 | bert_model.encoder.layer.8.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "161 | bert_model.encoder.layer.8.output.dropout              | Dropout              | 0     \n",
      "162 | bert_model.encoder.layer.9                             | BertLayer            | 7.1 M \n",
      "163 | bert_model.encoder.layer.9.attention                   | BertAttention        | 2.4 M \n",
      "164 | bert_model.encoder.layer.9.attention.self              | BertSelfAttention    | 1.8 M \n",
      "165 | bert_model.encoder.layer.9.attention.self.query        | Linear               | 590 K \n",
      "166 | bert_model.encoder.layer.9.attention.self.key          | Linear               | 590 K \n",
      "167 | bert_model.encoder.layer.9.attention.self.value        | Linear               | 590 K \n",
      "168 | bert_model.encoder.layer.9.attention.self.dropout      | Dropout              | 0     \n",
      "169 | bert_model.encoder.layer.9.attention.output            | BertSelfOutput       | 592 K \n",
      "170 | bert_model.encoder.layer.9.attention.output.dense      | Linear               | 590 K \n",
      "171 | bert_model.encoder.layer.9.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "172 | bert_model.encoder.layer.9.attention.output.dropout    | Dropout              | 0     \n",
      "173 | bert_model.encoder.layer.9.intermediate                | BertIntermediate     | 2.4 M \n",
      "174 | bert_model.encoder.layer.9.intermediate.dense          | Linear               | 2.4 M \n",
      "175 | bert_model.encoder.layer.9.output                      | BertOutput           | 2.4 M \n",
      "176 | bert_model.encoder.layer.9.output.dense                | Linear               | 2.4 M \n",
      "177 | bert_model.encoder.layer.9.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "178 | bert_model.encoder.layer.9.output.dropout              | Dropout              | 0     \n",
      "179 | bert_model.encoder.layer.10                            | BertLayer            | 7.1 M \n",
      "180 | bert_model.encoder.layer.10.attention                  | BertAttention        | 2.4 M \n",
      "181 | bert_model.encoder.layer.10.attention.self             | BertSelfAttention    | 1.8 M \n",
      "182 | bert_model.encoder.layer.10.attention.self.query       | Linear               | 590 K \n",
      "183 | bert_model.encoder.layer.10.attention.self.key         | Linear               | 590 K \n",
      "184 | bert_model.encoder.layer.10.attention.self.value       | Linear               | 590 K \n",
      "185 | bert_model.encoder.layer.10.attention.self.dropout     | Dropout              | 0     \n",
      "186 | bert_model.encoder.layer.10.attention.output           | BertSelfOutput       | 592 K \n",
      "187 | bert_model.encoder.layer.10.attention.output.dense     | Linear               | 590 K \n",
      "188 | bert_model.encoder.layer.10.attention.output.LayerNorm | LayerNorm            | 1.5 K \n",
      "189 | bert_model.encoder.layer.10.attention.output.dropout   | Dropout              | 0     \n",
      "190 | bert_model.encoder.layer.10.intermediate               | BertIntermediate     | 2.4 M \n",
      "191 | bert_model.encoder.layer.10.intermediate.dense         | Linear               | 2.4 M \n",
      "192 | bert_model.encoder.layer.10.output                     | BertOutput           | 2.4 M \n",
      "193 | bert_model.encoder.layer.10.output.dense               | Linear               | 2.4 M \n",
      "194 | bert_model.encoder.layer.10.output.LayerNorm           | LayerNorm            | 1.5 K \n",
      "195 | bert_model.encoder.layer.10.output.dropout             | Dropout              | 0     \n",
      "196 | bert_model.encoder.layer.11                            | BertLayer            | 7.1 M \n",
      "197 | bert_model.encoder.layer.11.attention                  | BertAttention        | 2.4 M \n",
      "198 | bert_model.encoder.layer.11.attention.self             | BertSelfAttention    | 1.8 M \n",
      "199 | bert_model.encoder.layer.11.attention.self.query       | Linear               | 590 K \n",
      "200 | bert_model.encoder.layer.11.attention.self.key         | Linear               | 590 K \n",
      "201 | bert_model.encoder.layer.11.attention.self.value       | Linear               | 590 K \n",
      "202 | bert_model.encoder.layer.11.attention.self.dropout     | Dropout              | 0     \n",
      "203 | bert_model.encoder.layer.11.attention.output           | BertSelfOutput       | 592 K \n",
      "204 | bert_model.encoder.layer.11.attention.output.dense     | Linear               | 590 K \n",
      "205 | bert_model.encoder.layer.11.attention.output.LayerNorm | LayerNorm            | 1.5 K \n",
      "206 | bert_model.encoder.layer.11.attention.output.dropout   | Dropout              | 0     \n",
      "207 | bert_model.encoder.layer.11.intermediate               | BertIntermediate     | 2.4 M \n",
      "208 | bert_model.encoder.layer.11.intermediate.dense         | Linear               | 2.4 M \n",
      "209 | bert_model.encoder.layer.11.output                     | BertOutput           | 2.4 M \n",
      "210 | bert_model.encoder.layer.11.output.dense               | Linear               | 2.4 M \n",
      "211 | bert_model.encoder.layer.11.output.LayerNorm           | LayerNorm            | 1.5 K \n",
      "212 | bert_model.encoder.layer.11.output.dropout             | Dropout              | 0     \n",
      "213 | bert_model.pooler                                      | BertPooler           | 590 K \n",
      "214 | bert_model.pooler.dense                                | Linear               | 590 K \n",
      "215 | bert_model.pooler.activation                           | Tanh                 | 0     \n",
      "216 | classifier                                             | SequenceClassifier   | 592 K \n",
      "217 | classifier.dropout                                     | Dropout              | 0     \n",
      "218 | classifier.mlp                                         | MultiLayerPerceptron | 592 K \n",
      "219 | classifier.mlp.layer0                                  | Linear               | 590 K \n",
      "220 | classifier.mlp.layer2                                  | Linear               | 1.5 K \n",
      "221 | loss                                                   | CrossEntropyLoss     | 0     \n",
      "222 | classification_report                                  | ClassificationReport | 0     \n",
      "--------------------------------------------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "Validation sanity check: 100% 2/2 [00:00<00:00,  1.63it/s][NeMo I 2022-03-05 05:44:19 text_classification_model:166] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             20.00       1.64       3.03         61\n",
      "    label_id: 1                                             51.22      94.03      66.32         67\n",
      "    -------------------\n",
      "    micro avg                                               50.00      50.00      50.00        128\n",
      "    macro avg                                               35.61      47.83      34.67        128\n",
      "    weighted avg                                            36.34      50.00      36.16        128\n",
      "    \n",
      "Epoch 0:  99% 1053/1067 [03:27<00:02,  5.08it/s, loss=0.188, val_loss=0.7, lr=1.11e-5]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0:  99% 1055/1067 [03:27<00:02,  5.08it/s, loss=0.188, val_loss=0.7, lr=1.11e-5]\n",
      "Epoch 0:  99% 1057/1067 [03:27<00:01,  5.09it/s, loss=0.188, val_loss=0.7, lr=1.11e-5]\n",
      "Epoch 0:  99% 1059/1067 [03:27<00:01,  5.10it/s, loss=0.188, val_loss=0.7, lr=1.11e-5]\n",
      "Epoch 0:  99% 1061/1067 [03:27<00:01,  5.10it/s, loss=0.188, val_loss=0.7, lr=1.11e-5]\n",
      "Epoch 0: 100% 1063/1067 [03:28<00:00,  5.11it/s, loss=0.188, val_loss=0.7, lr=1.11e-5]\n",
      "Epoch 0: 100% 1065/1067 [03:28<00:00,  5.11it/s, loss=0.188, val_loss=0.7, lr=1.11e-5]\n",
      "Epoch 0: 100% 1067/1067 [03:28<00:00,  5.12it/s, loss=0.188, val_loss=0.7, lr=1.11e-5][NeMo I 2022-03-05 05:47:47 text_classification_model:166] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             93.62      85.75      89.51        428\n",
      "    label_id: 1                                             87.29      94.37      90.69        444\n",
      "    -------------------\n",
      "    micro avg                                               90.14      90.14      90.14        872\n",
      "    macro avg                                               90.46      90.06      90.10        872\n",
      "    weighted avg                                            90.40      90.14      90.11        872\n",
      "    \n",
      "Epoch 0: 100% 1067/1067 [03:28<00:00,  5.12it/s, loss=0.188, val_loss=0.26, lr=1.11e-5]\n",
      "                                               \u001b[AEpoch 0, global step 1052: val_loss reached 0.26030 (best 0.26030), saving model to \"/workspace/mount/results/sst2/checkpoints/trained-model---val_loss=0.26-epoch=0.ckpt\" as top 3\n",
      "Epoch 0, global step 1052: val_loss reached 0.26030 (best 0.26030), saving model to \"/workspace/mount/results/sst2/checkpoints/trained-model---val_loss=0.26-epoch=0.ckpt\" as top 3\n",
      "Epoch 1:  99% 1053/1067 [03:27<00:02,  5.08it/s, loss=0.137, val_loss=0.26, lr=2.11e-8]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1:  99% 1055/1067 [03:27<00:02,  5.08it/s, loss=0.137, val_loss=0.26, lr=2.11e-8]\n",
      "Epoch 1:  99% 1057/1067 [03:27<00:01,  5.09it/s, loss=0.137, val_loss=0.26, lr=2.11e-8]\n",
      "Epoch 1:  99% 1059/1067 [03:27<00:01,  5.10it/s, loss=0.137, val_loss=0.26, lr=2.11e-8]\n",
      "Epoch 1:  99% 1061/1067 [03:27<00:01,  5.10it/s, loss=0.137, val_loss=0.26, lr=2.11e-8]\n",
      "Epoch 1: 100% 1063/1067 [03:28<00:00,  5.11it/s, loss=0.137, val_loss=0.26, lr=2.11e-8]\n",
      "Epoch 1: 100% 1065/1067 [03:28<00:00,  5.11it/s, loss=0.137, val_loss=0.26, lr=2.11e-8]\n",
      "Epoch 1: 100% 1067/1067 [03:28<00:00,  5.12it/s, loss=0.137, val_loss=0.26, lr=2.11e-8][NeMo I 2022-03-05 05:51:40 text_classification_model:166] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             92.01      88.79      90.37        428\n",
      "    label_id: 1                                             89.54      92.57      91.03        444\n",
      "    -------------------\n",
      "    micro avg                                               90.71      90.71      90.71        872\n",
      "    macro avg                                               90.78      90.68      90.70        872\n",
      "    weighted avg                                            90.75      90.71      90.71        872\n",
      "    \n",
      "Epoch 1: 100% 1067/1067 [03:28<00:00,  5.12it/s, loss=0.137, val_loss=0.248, lr=1.05e-8]\n",
      "                                               \u001b[AEpoch 1, global step 2105: val_loss reached 0.24794 (best 0.24794), saving model to \"/workspace/mount/results/sst2/checkpoints/trained-model---val_loss=0.25-epoch=1.ckpt\" as top 3\n",
      "Epoch 1, global step 2105: val_loss reached 0.24794 (best 0.24794), saving model to \"/workspace/mount/results/sst2/checkpoints/trained-model---val_loss=0.25-epoch=1.ckpt\" as top 3\n",
      "Epoch 1: 100% 1067/1067 [03:48<00:00,  4.67it/s, loss=0.137, val_loss=0.248, lr=1.05e-8]Saving latest checkpoint...\n",
      "Saving latest checkpoint...\n",
      "Epoch 1: 100% 1067/1067 [03:57<00:00,  4.49it/s, loss=0.137, val_loss=0.248, lr=1.05e-8]\n",
      "[NeMo I 2022-03-05 05:52:31 train:128] Experiment logs saved to '/workspace/mount/results/sst2'\n",
      "[NeMo I 2022-03-05 05:52:31 train:129] Trained model saved to '/workspace/mount/results/sst2/checkpoints/trained-model.tlt'\n",
      "2022-03-05 05:52:32,440 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n",
      "CPU times: user 8.51 s, sys: 2.06 s, total: 10.6 s\n",
      "Wall time: 9min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# For BERT training on SST-2:\n",
    "!tao text_classification train \\\n",
    "    -e $SPECS_DIR/text_classification/train.yaml \\\n",
    "    -g 1  \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/sst2 \\\n",
    "    training_ds.file_path=$DATA_DIR/SST-2/train.tsv \\\n",
    "    validation_ds.file_path=$DATA_DIR/SST-2/dev.tsv \\\n",
    "    model.class_labels.class_labels_file=$DATA_DIR/SST-2/label_ids.csv \\\n",
    "    trainer.amp_level='O1' \\\n",
    "    trainer.precision=16 \\\n",
    "    trainer.max_epochs=2 \\\n",
    "    2>&1|tee my_assessment/step2.txt # DO NOT REMOVE THIS LINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train command produces a model file called `trained-model.tlt` saved at `results/sst2/checkpoints/trained-model.tlt`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 3: Infer and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Queries (not graded)\n",
    "Execute the following cell to create queries for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /dli/task/tao/specs/text_classification/infer.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $SOURCE_MOUNT/specs/text_classification/infer.yaml\n",
    "\n",
    "# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n",
    "# TAO Spec file for inference using a previously pretrained BERT model for a text classification task.\n",
    "\n",
    "# \"Simulate\" user input: batch with four samples.\n",
    "input_batch:\n",
    "- \"this is a good script , good dialogue , funny even for adults .\"\n",
    "- \"the affectionate loopiness that once seemed congenital to demme s perspective has a tough time emerging from between the badly dated cutesy-pie mystery scenario a nd the newfangled hollywood post-production effects .\"\n",
    "- \" this piece of channel 5 grade trash is , quite frankly , an insult to the intelligence of the true genre enthusiast . \"\n",
    "- \"a delightful coming-of-age story .\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference on the Trained Model (not graded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-05 05:57:53,423 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-03-05 05:57:53,577 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2022-03-05 05:57:57 experimental:27] Module <class 'nemo.collections.nlp.modules.common.megatron.megatron_bert.MegatronBertEncoder'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "INFO: Generating new fontManager, this may take some time...\n",
      "[NeMo W 2022-03-05 05:58:01 experimental:27] Module <class 'nemo.collections.nlp.modules.common.megatron.megatron_bert.MegatronBertEncoder'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo I 2022-03-05 05:58:01 tlt_logging:20] Experiment configuration:\n",
      "    restore_from: /workspace/mount/results/sst2/checkpoints/trained-model.tlt\n",
      "    exp_manager:\n",
      "      task_name: infer\n",
      "      explicit_log_dir: /workspace/mount/results/sst2/infer\n",
      "    input_batch:\n",
      "    - this is a good script , good dialogue , funny even for adults .\n",
      "    - the affectionate loopiness that once seemed congenital to demme s perspective has\n",
      "      a tough time emerging from between the badly dated cutesy-pie mystery scenario a\n",
      "      nd the newfangled hollywood post-production effects .\n",
      "    - ' this piece of channel 5 grade trash is , quite frankly , an insult to the intelligence\n",
      "      of the true genre enthusiast . '\n",
      "    - a delightful coming-of-age story .\n",
      "    encryption_key: '***'\n",
      "    \n",
      "[NeMo W 2022-03-05 05:58:03 modelPT:193] Using /tmp/tmp8xhrq0sl/tokenizer.vocab_file instead of tokenizer.vocab_file.\n",
      "Lock 140623984880560 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Downloading: 100%|| 570/570 [00:00<00:00, 602kB/s]\n",
      "Lock 140623984880560 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Lock 140623984880032 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Downloading: 100%|| 232k/232k [00:00<00:00, 66.3MB/s]\n",
      "Lock 140623984880032 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Lock 140623985079632 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Downloading: 100%|| 28.0/28.0 [00:00<00:00, 36.6kB/s]\n",
      "Lock 140623985079632 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Lock 140623984878976 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Downloading: 100%|| 466k/466k [00:00<00:00, 72.2MB/s]\n",
      "Lock 140623984878976 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2022-03-05 05:58:04 modelPT:1202] World size can only be set by PyTorch Lightning Trainer.\n",
      "Lock 140623984880224 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Downloading: 100%|| 440M/440M [00:05<00:00, 86.8MB/s]\n",
      "Lock 140623984880224 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "[NeMo W 2022-03-05 05:58:12 modelPT:193] Using /tmp/tmp8xhrq0sl/label_ids.csv instead of /workspace/mount/data/SST-2/label_ids.csv.\n",
      "[NeMo I 2022-03-05 05:58:20 infer:103] The prediction results of some sample queries with the trained model:\n",
      "[NeMo I 2022-03-05 05:58:20 infer:105] Query: this is a good script , good dialogue , funny even for adults .\n",
      "[NeMo I 2022-03-05 05:58:20 infer:106] Predicted label: positive\n",
      "[NeMo I 2022-03-05 05:58:20 infer:105] Query: the affectionate loopiness that once seemed congenital to demme s perspective has a tough time emerging from between the badly dated cutesy-pie mystery scenario a nd the newfangled hollywood post-production effects .\n",
      "[NeMo I 2022-03-05 05:58:20 infer:106] Predicted label: negative\n",
      "[NeMo I 2022-03-05 05:58:20 infer:105] Query:  this piece of channel 5 grade trash is , quite frankly , an insult to the intelligence of the true genre enthusiast . \n",
      "[NeMo I 2022-03-05 05:58:20 infer:106] Predicted label: negative\n",
      "[NeMo I 2022-03-05 05:58:20 infer:105] Query: a delightful coming-of-age story .\n",
      "[NeMo I 2022-03-05 05:58:20 infer:106] Predicted label: positive\n",
      "[NeMo I 2022-03-05 05:58:20 infer:109] Experiment logs saved to '/workspace/mount/results/sst2/infer'\n",
      "2022-03-05 05:58:21,931 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# Run inference on user data:\n",
    "!tao text_classification infer \\\n",
    "    -e $SPECS_DIR/text_classification/infer.yaml \\\n",
    "    -g 1 \\\n",
    "    -m $RESULTS_DIR/sst2/checkpoints/trained-model.tlt \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/sst2/infer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate your Model (results graded)\n",
    "Execute the following cell without changes.  Review your output to see if you had an F1 result above the 88% goal.  If not, you may need to retrain your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-05 05:58:32,986 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-03-05 05:58:33,082 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2022-03-05 05:58:36 experimental:27] Module <class 'nemo.collections.nlp.modules.common.megatron.megatron_bert.MegatronBertEncoder'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "INFO: Generating new fontManager, this may take some time...\n",
      "[NeMo W 2022-03-05 05:58:40 experimental:27] Module <class 'nemo.collections.nlp.modules.common.megatron.megatron_bert.MegatronBertEncoder'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo I 2022-03-05 05:58:40 tlt_logging:20] Experiment configuration:\n",
      "    restore_from: /workspace/mount/results/sst2/checkpoints/trained-model.tlt\n",
      "    exp_manager:\n",
      "      explicit_log_dir: /workspace/mount/results/sst2/eval\n",
      "      exp_dir: null\n",
      "      name: null\n",
      "      version: null\n",
      "      use_datetime_version: true\n",
      "      resume_if_exists: false\n",
      "      resume_past_end: false\n",
      "      resume_ignore_no_checkpoint: false\n",
      "      create_tensorboard_logger: false\n",
      "      summary_writer_kwargs: null\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs: null\n",
      "      create_checkpoint_callback: false\n",
      "      checkpoint_callback_params:\n",
      "        filepath: null\n",
      "        monitor: val_loss\n",
      "        verbose: true\n",
      "        save_last: true\n",
      "        save_top_k: 3\n",
      "        save_weights_only: false\n",
      "        mode: auto\n",
      "        period: 1\n",
      "        prefix: null\n",
      "        postfix: .nemo\n",
      "        save_best_model: false\n",
      "      files_to_copy: null\n",
      "    trainer:\n",
      "      logger: false\n",
      "      checkpoint_callback: false\n",
      "      callbacks: null\n",
      "      default_root_dir: null\n",
      "      gradient_clip_val: 0.0\n",
      "      process_position: 0\n",
      "      num_nodes: 1\n",
      "      num_processes: 1\n",
      "      gpus: 1\n",
      "      auto_select_gpus: false\n",
      "      tpu_cores: null\n",
      "      log_gpu_memory: null\n",
      "      progress_bar_refresh_rate: 1\n",
      "      overfit_batches: 0.0\n",
      "      track_grad_norm: -1\n",
      "      check_val_every_n_epoch: 1\n",
      "      fast_dev_run: false\n",
      "      accumulate_grad_batches: 1\n",
      "      max_epochs: 1000\n",
      "      min_epochs: 1\n",
      "      max_steps: null\n",
      "      min_steps: null\n",
      "      limit_train_batches: 1.0\n",
      "      limit_val_batches: 1.0\n",
      "      limit_test_batches: 1.0\n",
      "      val_check_interval: 1.0\n",
      "      flush_logs_every_n_steps: 100\n",
      "      log_every_n_steps: 50\n",
      "      accelerator: ddp\n",
      "      sync_batchnorm: false\n",
      "      precision: 32\n",
      "      weights_summary: full\n",
      "      weights_save_path: null\n",
      "      num_sanity_val_steps: 2\n",
      "      truncated_bptt_steps: null\n",
      "      resume_from_checkpoint: null\n",
      "      profiler: null\n",
      "      benchmark: false\n",
      "      deterministic: false\n",
      "      reload_dataloaders_every_epoch: false\n",
      "      auto_lr_find: false\n",
      "      replace_sampler_ddp: true\n",
      "      terminate_on_nan: false\n",
      "      auto_scale_batch_size: false\n",
      "      prepare_data_per_node: true\n",
      "      amp_backend: native\n",
      "      amp_level: O2\n",
      "    test_ds:\n",
      "      file_path: /workspace/mount/data/SST-2/test.tsv\n",
      "      batch_size: 32\n",
      "      shuffle: false\n",
      "      num_samples: -1\n",
      "      num_workers: 3\n",
      "      drop_last: false\n",
      "      pin_memory: false\n",
      "    encryption_key: '*******'\n",
      "    \n",
      "GPU available: True, used: True\n",
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2022-03-05 05:58:40 exp_manager:194] Experiments will be logged at /workspace/mount/results/sst2/eval\n",
      "[NeMo W 2022-03-05 05:58:42 modelPT:193] Using /tmp/tmpgcychfds/tokenizer.vocab_file instead of tokenizer.vocab_file.\n",
      "Lock 139704589692592 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Downloading: 100% 570/570 [00:00<00:00, 637kB/s]\n",
      "Lock 139704589692592 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Lock 139704590419952 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Downloading: 100% 232k/232k [00:00<00:00, 55.6MB/s]\n",
      "Lock 139704590419952 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Lock 139704590418608 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Downloading: 100% 28.0/28.0 [00:00<00:00, 35.6kB/s]\n",
      "Lock 139704590418608 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Lock 139704590382896 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Downloading: 100% 466k/466k [00:00<00:00, 68.0MB/s]\n",
      "Lock 139704590382896 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2022-03-05 05:58:43 modelPT:1202] World size can only be set by PyTorch Lightning Trainer.\n",
      "Lock 139704590382032 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Downloading: 100% 440M/440M [00:05<00:00, 86.0MB/s] \n",
      "Lock 139704590382032 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "[NeMo W 2022-03-05 05:58:51 modelPT:193] Using /tmp/tmpgcychfds/label_ids.csv instead of /workspace/mount/data/SST-2/label_ids.csv.\n",
      "[NeMo I 2022-03-05 05:58:57 text_classification_dataset:120] Read 100 examples from /workspace/mount/data/SST-2/test.tsv.\n",
      "[NeMo I 2022-03-05 05:58:57 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2022-03-05 05:58:57 text_classification_dataset:239] example 0: ['not', 'the', 'kind', 'of', 'film', 'that', 'will', 'appeal', 'to', 'a', 'mainstream', 'american', 'audience', ',', 'but', 'there', 'is', 'a', 'certain', 'charm', 'about', 'the', 'film', 'that', 'makes', 'it', 'a', 'suitable', 'entry', 'into', 'the', 'fest', 'circuit', '.']\n",
      "[NeMo I 2022-03-05 05:58:57 text_classification_dataset:240] subtokens: [CLS] not the kind of film that will appeal to a mainstream american audience , but there is a certain charm about the film that makes it a suitable entry into the fest circuit . [SEP]\n",
      "[NeMo I 2022-03-05 05:58:57 text_classification_dataset:241] input_ids: 101 2025 1996 2785 1997 2143 2008 2097 5574 2000 1037 7731 2137 4378 1010 2021 2045 2003 1037 3056 11084 2055 1996 2143 2008 3084 2009 1037 7218 4443 2046 1996 17037 4984 1012 102\n",
      "[NeMo I 2022-03-05 05:58:57 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-03-05 05:58:57 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2022-03-05 05:58:57 text_classification_dataset:244] label: 1\n",
      "[NeMo I 2022-03-05 05:58:57 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2022-03-05 05:58:57 text_classification_dataset:239] example 1: ['it', \"'s\", 'a', 'beautiful', 'madness', '.']\n",
      "[NeMo I 2022-03-05 05:58:57 text_classification_dataset:240] subtokens: [CLS] it ' s a beautiful madness . [SEP]\n",
      "[NeMo I 2022-03-05 05:58:57 text_classification_dataset:241] input_ids: 101 2009 1005 1055 1037 3376 12013 1012 102\n",
      "[NeMo I 2022-03-05 05:58:57 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-03-05 05:58:57 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2022-03-05 05:58:57 text_classification_dataset:244] label: 1\n",
      "[NeMo I 2022-03-05 05:58:58 data_preprocessing:295] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2022-03-05 05:58:58 data_preprocessing:297] Min: 7 |                  Max: 54 |                  Mean: 25.85 |                  Median: 25.0\n",
      "[NeMo I 2022-03-05 05:58:58 data_preprocessing:303] 75 percentile: 34.00\n",
      "[NeMo I 2022-03-05 05:58:58 data_preprocessing:304] 99 percentile: 52.02\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "Testing:  50% 2/4 [00:00<00:01,  1.55it/s][NeMo I 2022-03-05 05:58:59 text_classification_model:166] test_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             97.56      83.33      89.89         48\n",
      "    label_id: 1                                             86.44      98.08      91.89         52\n",
      "    -------------------\n",
      "    micro avg                                               91.00      91.00      91.00        100\n",
      "    macro avg                                               92.00      90.71      90.89        100\n",
      "    weighted avg                                            91.78      91.00      90.93        100\n",
      "    \n",
      "Testing: 100% 4/4 [00:01<00:00,  3.50it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_f1': tensor(91., device='cuda:0'),\n",
      " 'test_loss': tensor(0.1894, device='cuda:0'),\n",
      " 'test_precision': tensor(91., device='cuda:0'),\n",
      " 'test_recall': tensor(91., device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "[NeMo I 2022-03-05 05:58:59 evaluate:97] Experiment logs saved to '/workspace/mount/results/sst2/eval'\n",
      "2022-03-05 05:59:00,567 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# For BERT evaluation on SST-2:\n",
    "!tao text_classification evaluate \\\n",
    "    -e $SPECS_DIR/text_classification/evaluate.yaml \\\n",
    "    -g 1 \\\n",
    "    -m $RESULTS_DIR/sst2/checkpoints/trained-model.tlt \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/sst2/eval \\\n",
    "    test_ds.file_path=$DATA_DIR/SST-2/test.tsv \\\n",
    "    test_ds.batch_size=32 \\\n",
    "    test_ds.num_samples=-1 \\\n",
    "    2>&1|tee my_assessment/step3.txt # DO NOT REMOVE THIS LINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 4: Export Custom Model\n",
    "### Export the Model for Riva (graded)\n",
    "Complete the <i><strong style=\"color:green;\">#FIXME</strong></i> line(s) and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-05 05:59:59,759 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-03-05 05:59:59,872 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2022-03-05 06:00:03 experimental:27] Module <class 'nemo.collections.nlp.modules.common.megatron.megatron_bert.MegatronBertEncoder'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "INFO: Generating new fontManager, this may take some time...\n",
      "[NeMo W 2022-03-05 06:00:06 experimental:27] Module <class 'nemo.collections.nlp.modules.common.megatron.megatron_bert.MegatronBertEncoder'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo I 2022-03-05 06:00:07 tlt_logging:20] Experiment configuration:\n",
      "    restore_from: /workspace/mount/results/sst2/checkpoints/trained-model.tlt\n",
      "    export_to: tc-model.riva\n",
      "    export_format: RIVA\n",
      "    exp_manager:\n",
      "      task_name: export\n",
      "      explicit_log_dir: /workspace/mount/results/sst2/export/\n",
      "    encryption_key: '******'\n",
      "    \n",
      "[NeMo W 2022-03-05 06:00:09 modelPT:193] Using /tmp/tmp1v5oobk9/tokenizer.vocab_file instead of tokenizer.vocab_file.\n",
      "Lock 140639234505600 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Downloading: 100% 570/570 [00:00<00:00, 672kB/s]\n",
      "Lock 140639234505600 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Lock 140639234504112 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Downloading: 100% 232k/232k [00:00<00:00, 62.6MB/s]\n",
      "Lock 140639234504112 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Lock 140639234517840 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Downloading: 100% 28.0/28.0 [00:00<00:00, 31.6kB/s]\n",
      "Lock 140639234517840 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Lock 140639234504832 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Downloading: 100% 466k/466k [00:00<00:00, 71.9MB/s]\n",
      "Lock 140639234504832 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2022-03-05 06:00:09 modelPT:1202] World size can only be set by PyTorch Lightning Trainer.\n",
      "Lock 140639234505456 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Downloading: 100% 440M/440M [00:04<00:00, 94.4MB/s] \n",
      "Lock 140639234505456 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "[NeMo W 2022-03-05 06:00:18 modelPT:193] Using /tmp/tmp1v5oobk9/label_ids.csv instead of /workspace/mount/data/SST-2/label_ids.csv.\n",
      "[NeMo I 2022-03-05 06:00:24 export:54] Model restored from '/workspace/mount/results/sst2/checkpoints/trained-model.tlt'\n",
      "Could not retrieve the artifact tokenizer.vocab_file used in tokenizer.vocab_file\n",
      "Could not retrieve the artifact label_ids.csv used in class_labels\n",
      "[NeMo I 2022-03-05 06:00:47 export:77] Experiment logs saved to '/workspace/mount/results/sst2/export'\n",
      "[NeMo I 2022-03-05 06:00:47 export:78] Exported model to '/workspace/mount/results/sst2/export/tc-model.riva'\n",
      "[NeMo I 2022-03-05 06:00:48 export:89] Exported model is compliant with Riva\n",
      "2022-03-05 06:00:50,158 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "#  For export to Riva:\n",
    "!tao text_classification export \\\n",
    "    -e $SPECS_DIR/text_classification/export.yaml \\\n",
    "    -g 1 \\\n",
    "    -m $RESULTS_DIR/sst2/checkpoints/trained-model.tlt \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/sst2/export/ \\\n",
    "    export_format=RIVA \\\n",
    "    export_to=tc-model.riva \\\n",
    "    2>&1|tee my_assessment/step4.txt # DO NOT REMOVE THIS LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export.log  tc-model.riva\n"
     ]
    }
   ],
   "source": [
    "# Check your work - does the exported tc-model.riva model exist?\n",
    "!ls $EXPORT_MODEL_LOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 5: Build and Deploy with Riva\n",
    "### Set up Project Paths (not graded)\n",
    "This block is complete, but feel free to add to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Riva paths for the project\n",
    "WORKSPACE = \"/dli/task\"\n",
    "\n",
    "##### Riva Paths\n",
    "# ServiceMaker Docker\n",
    "RIVA_SM_CONTAINER = \"nvcr.io/nvidia/riva/riva-speech:1.4.0-beta-servicemaker\"\n",
    "\n",
    "# Model output directories\n",
    "RMIR_LOC = WORKSPACE + \"/riva/riva_quickstart/models_repo_assessment/rmir\"\n",
    "RIVA_MODEL_LOC = WORKSPACE + '/riva/riva_quickstart/models_repo_assessment'\n",
    "\n",
    "# Model Names\n",
    "EXPORT_MODEL_NAME = \"tc-model.riva\"  \n",
    "RMIR_MODEL_NAME = \"tc-model.rmir\"\n",
    "\n",
    "# Riva Quick Start \n",
    "RIVA_QS = WORKSPACE + \"/riva/riva_quickstart\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Deploy with Riva ServiceMaker (graded)\n",
    "Complete the <i><strong style=\"color:green;\">#FIXME</strong></i> line(s) and run the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      "=== Riva Speech Skills ===\n",
      "==========================\n",
      "\n",
      "NVIDIA Release devel (build 22382700)\n",
      "\n",
      "Copyright (c) 2018-2021, NVIDIA CORPORATION.  All rights reserved.\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\n",
      "NVIDIA modifications are covered by the license terms that apply to the underlying\n",
      "project or file.\n",
      "\n",
      "NOTE: Legacy NVIDIA Driver detected.  Compatibility mode ENABLED.\n",
      "\n",
      "NOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n",
      "   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n",
      "   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n",
      "\n",
      "2022-03-05 06:03:47,638 [ERROR] Condition for key 'runtime' (PyTorch  <built-in function eq> ONNX) is not fulfilled\n",
      "2022-03-05 06:03:47,705 [INFO] Packing binaries for self\n",
      "2022-03-05 06:03:48,886 [ERROR] Condition for key 'runtime' (PyTorch  <built-in function eq> ONNX) is not fulfilled\n",
      "2022-03-05 06:03:48,949 [INFO] Trying to extract from model tc-model.riva\n",
      "2022-03-05 06:03:50,194 [INFO] Packing binaries for language_model\n",
      "2022-03-05 06:03:51,382 [ERROR] Condition for key 'runtime' (PyTorch  <built-in function eq> ONNX) is not fulfilled\n",
      "2022-03-05 06:03:51,446 [INFO] Trying to extract from model tc-model.riva\n",
      "2022-03-05 06:03:57,647 [ERROR] Condition for key 'runtime' (PyTorch  <built-in function eq> ONNX) is not fulfilled\n",
      "2022-03-05 06:03:57,710 [INFO] Trying to extract from model tc-model.riva\n",
      "2022-03-05 06:03:58,963 [INFO] Packing binaries for tokenizer\n",
      "2022-03-05 06:04:00,151 [ERROR] Condition for key 'runtime' (PyTorch  <built-in function eq> ONNX) is not fulfilled\n",
      "2022-03-05 06:04:00,215 [INFO] Trying to extract from model tc-model.riva\n"
     ]
    }
   ],
   "source": [
    "# Syntax: riva-build <task-name> output-dir-for-rmir/model.rmir:key dir-for-riva/model.riva:key\n",
    "!docker run --rm --gpus 1 \\\n",
    "    -v $EXPORT_MODEL_LOC:/tao \\\n",
    "    -v $RMIR_LOC:/riva \\\n",
    "    $RIVA_SM_CONTAINER -- \\\n",
    "    riva-build text_classification \\\n",
    "    -f /riva/$RMIR_MODEL_NAME:$KEY /tao/$EXPORT_MODEL_NAME:$KEY \\\n",
    "    2>&1|tee my_assessment/step5.txt # DO NOT REMOVE THIS LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tc-model.rmir\n"
     ]
    }
   ],
   "source": [
    "# Check your work - does the exported tc-model.rmir model exist?\n",
    "!ls $RMIR_LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      "=== Riva Speech Skills ===\n",
      "==========================\n",
      "\n",
      "NVIDIA Release devel (build 22382700)\n",
      "\n",
      "Copyright (c) 2018-2021, NVIDIA CORPORATION.  All rights reserved.\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\n",
      "NVIDIA modifications are covered by the license terms that apply to the underlying\n",
      "project or file.\n",
      "\n",
      "NOTE: Legacy NVIDIA Driver detected.  Compatibility mode ENABLED.\n",
      "\n",
      "NOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n",
      "   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n",
      "   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n",
      "\n",
      "2022-03-05 06:07:07,471 [INFO] Writing Riva model repository to '/data/models/'...\n",
      "2022-03-05 06:07:07,471 [INFO] The riva model repo target directory is /data/models/\n",
      "2022-03-05 06:07:08,665 [INFO] Extract_binaries for tokenizer -> /data/models/riva_tokenizer/1\n",
      "2022-03-05 06:07:08,667 [INFO] Extract_binaries for language_model -> /data/models/riva-trt-riva_text_classification_default-nn-bert-base-uncased/1\n",
      "2022-03-05 06:07:12,844 [INFO] Printing copied artifacts:\n",
      "2022-03-05 06:07:12,844 [INFO] {'ckpt': '/data/models/riva-trt-riva_text_classification_default-nn-bert-base-uncased/1/model_weights.ckpt', 'bert_config_file': '/data/models/riva-trt-riva_text_classification_default-nn-bert-base-uncased/1/bert-base-uncased_encoder_config.json'}\n",
      "2022-03-05 06:07:12,844 [INFO] Building TRT engine from PyTorch Checkpoint\n",
      "2022-03-05 06:08:41,813 [INFO] Text Classification classes:2\n",
      "2022-03-05 06:08:41,814 [INFO] Extract_binaries for self -> /data/models/riva_text_classification_default/1\n"
     ]
    }
   ],
   "source": [
    "# Syntax: riva-deploy -f dir-for-rmir/model.rmir:key output-dir-for-repository\n",
    "!docker run --rm --gpus 1 \\\n",
    "    -v $RIVA_MODEL_LOC:/data \\\n",
    "    $RIVA_SM_CONTAINER -- \\\n",
    "    riva-deploy -f  /data/rmir/$RMIR_MODEL_NAME:$KEY \\\n",
    "    /data/models/ \\\n",
    "    2>&1|tee -a my_assessment/step5.txt # DO NOT REMOVE THIS LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "riva-trt-riva_text_classification_default-nn-bert-base-uncased\triva_tokenizer\n",
      "riva_text_classification_default\n"
     ]
    }
   ],
   "source": [
    "# Check your work - are there optimized models for text classification?\n",
    "!ls $RIVA_MODEL_LOC/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 6: Start Riva Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and Start Riva (graded)\n",
    "Next, modify the [config.sh](riva/riva_quickstart/config.sh) to enable relevant Riva services. \n",
    "In this case, we want to start NLP services, provide the encryption key, and update the path to the model repository (`RIVA_MODEL_LOC`). \n",
    "Open the [config.sh](riva/riva_quickstart/config.sh) and make changes where necessary, then start the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Riva Speech Services. This may take several minutes depending on the number of models deployed.\n",
      "Waiting for Riva server to load all models...retrying in 10 seconds\n",
      "Waiting for Riva server to load all models...retrying in 10 seconds\n",
      "Riva server is ready...\n"
     ]
    }
   ],
   "source": [
    "# Run Riva Start. This will deploy the model.\n",
    "!cd $RIVA_QS && bash riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      "=== Riva Speech Skills ===\n",
      "==========================\n",
      "\n",
      "NVIDIA Release 21.07 (build 25292380)\n",
      "\n",
      "Copyright (c) 2018-2021, NVIDIA CORPORATION.  All rights reserved.\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\n",
      "NVIDIA modifications are covered by the license terms that apply to the underlying\n",
      "project or file.\n",
      "\n",
      "NOTE: Legacy NVIDIA Driver detected.  Compatibility mode ENABLED.\n",
      "\n",
      "NOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n",
      "   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n",
      "   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n",
      "\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "I0305 06:10:10.043343 73 metrics.cc:228] Collecting metrics for GPU 0: Tesla T4\n",
      "I0305 06:10:10.092016 73 onnxruntime.cc:1722] TRITONBACKEND_Initialize: onnxruntime\n",
      "I0305 06:10:10.093050 73 onnxruntime.cc:1732] Triton TRITONBACKEND API version: 1.0\n",
      "I0305 06:10:10.093065 73 onnxruntime.cc:1738] 'onnxruntime' TRITONBACKEND API version: 1.0\n",
      "I0305 06:10:10.306339 73 pinned_memory_manager.cc:206] Pinned memory pool is created at '0x7ff430000000' with size 268435456\n",
      "I0305 06:10:10.307593 73 cuda_memory_manager.cc:103] CUDA memory pool is created on device 0 with size 1000000000\n",
      "I0305 06:10:10.316373 73 model_repository_manager.cc:1066] loading: riva_tokenizer:1\n",
      "I0305 06:10:10.416679 73 model_repository_manager.cc:1066] loading: riva-trt-riva_text_classification_default-nn-bert-base-uncased:1\n",
      "I0305 06:10:10.416992 73 custom_backend.cc:198] Creating instance riva_tokenizer_0_0_cpu on CPU using libtriton_riva_nlp_tokenizer.so\n",
      "I0305 06:10:10.467853 73 model_repository_manager.cc:1240] successfully loaded 'riva_tokenizer' version 1\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "I0305 06:10:26.344939 73 plan_backend.cc:384] Creating instance riva-trt-riva_text_classification_default-nn-bert-base-uncased_0_0_gpu0 on GPU 0 (7.5) using model.plan\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "I0305 06:10:27.745851 73 plan_backend.cc:768] Created instance riva-trt-riva_text_classification_default-nn-bert-base-uncased_0_0_gpu0 on GPU 0 with stream priority 0 and optimization profile default[0];\n",
      "I0305 06:10:27.756663 73 model_repository_manager.cc:1240] successfully loaded 'riva-trt-riva_text_classification_default-nn-bert-base-uncased' version 1\n",
      "I0305 06:10:27.756904 73 model_repository_manager.cc:1066] loading: riva_text_classification_default:1\n",
      "I0305 06:10:27.857269 73 model_repository_manager.cc:1240] successfully loaded 'riva_text_classification_default' version 1\n",
      "I0305 06:10:27.857375 73 server.cc:504] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0305 06:10:27.857429 73 server.cc:543] \n",
      "+-------------+-----------------------------------------------------------------+--------+\n",
      "| Backend     | Path                                                            | Config |\n",
      "+-------------+-----------------------------------------------------------------+--------+\n",
      "| tensorrt    | <built-in>                                                      | {}     |\n",
      "| onnxruntime | /opt/tritonserver/backends/onnxruntime/libtriton_onnxruntime.so | {}     |\n",
      "+-------------+-----------------------------------------------------------------+--------+\n",
      "\n",
      "I0305 06:10:27.857494 73 server.cc:586] \n",
      "+----------------------------------------------------------------+---------+--------+\n",
      "| Model                                                          | Version | Status |\n",
      "+----------------------------------------------------------------+---------+--------+\n",
      "| riva-trt-riva_text_classification_default-nn-bert-base-uncased | 1       | READY  |\n",
      "| riva_text_classification_default                               | 1       | READY  |\n",
      "| riva_tokenizer                                                 | 1       | READY  |\n",
      "+----------------------------------------------------------------+---------+--------+\n",
      "\n",
      "I0305 06:10:27.857615 73 tritonserver.cc:1658] \n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Option                           | Value                                                                                                                                                                                  |\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| server_id                        | triton                                                                                                                                                                                 |\n",
      "| server_version                   | 2.9.0                                                                                                                                                                                  |\n",
      "| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics |\n",
      "| model_repository_path[0]         | /data/models                                                                                                                                                                           |\n",
      "| model_control_mode               | MODE_NONE                                                                                                                                                                              |\n",
      "| strict_model_config              | 1                                                                                                                                                                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                              |\n",
      "| cuda_memory_pool_byte_size{0}    | 1000000000                                                                                                                                                                             |\n",
      "| min_supported_compute_capability | 6.0                                                                                                                                                                                    |\n",
      "| strict_readiness                 | 1                                                                                                                                                                                      |\n",
      "| exit_timeout                     | 30                                                                                                                                                                                     |\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "I0305 06:10:27.870021 73 grpc_server.cc:4028] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I0305 06:10:27.872558 73 http_server.cc:2761] Started HTTPService at 0.0.0.0:8000\n",
      "I0305 06:10:27.914510 73 http_server.cc:2780] Started Metrics Service at 0.0.0.0:8002\n",
      "  > Triton server is ready...\n",
      "I0305 06:10:28.162902   173 grpc_health.cc:27] RivaHealthService initialized with server: localhost:8001\n",
      "I0305 06:10:28.164754   173 client.cc:38] RivaLanguageUnderstandingClient initialized with server: localhost:8001\n",
      "I0305 06:10:28.175573   173 client.cc:54] Our model repository has: 3 models.\n",
      "I0305 06:10:28.186434   173 client.cc:72] Registering 'riva_text_classification_default' with service '/nvidia.riva.nlp.RivaLanguageUnderstanding/ClassifyText'\n",
      "I0305 06:10:28.187173   173 model_registry.cc:36] RivaModelRegistry initialized with server: localhost:8001\n",
      "I0305 06:10:28.187511   173 model_registry.cc:65] Server Name: triton, Server version: 2.9.0\n",
      "I0305 06:10:28.187736   173 model_registry.cc:86] Our model repository has a total of: 3 models\n",
      "I0305 06:10:28.187749   173 model_registry.cc:91] Model names: riva-trt-riva_text_classification_default-nn-bert-base-uncased, Model version: 1\n",
      "I0305 06:10:28.188357   173 model_registry.cc:91] Model names: riva_text_classification_default, Model version: 1\n",
      "I0305 06:10:28.188983   173 model_registry.cc:104] 'Successfully registering riva_text_classification_default'\n",
      "I0305 06:10:28.189013   173 model_registry.cc:91] Model names: riva_tokenizer, Model version: 1\n",
      "I0305 06:10:28.189678   173 model_registry.cc:109] Successfully registered: 1 models.\n",
      "I0305 06:10:28.189697   173 grpc_riva_nlp.cc:33] NLPService GRPC service started\n",
      "I0305 06:10:28.189702   173 client.cc:38] RivaLanguageUnderstandingClient initialized with server: localhost:8001\n",
      "I0305 06:10:28.189893   173 client.cc:54] Our model repository has: 3 models.\n",
      "I0305 06:10:28.191097   173 client.cc:72] Registering 'riva_text_classification_default' with service '/nvidia.riva.nlp.RivaLanguageUnderstanding/ClassifyText'\n",
      "I0305 06:10:28.191749   173 model_registry.cc:36] RivaModelRegistry initialized with server: localhost:8001\n",
      "I0305 06:10:28.192057   173 model_registry.cc:65] Server Name: triton, Server version: 2.9.0\n",
      "I0305 06:10:28.192240   173 model_registry.cc:86] Our model repository has a total of: 3 models\n",
      "I0305 06:10:28.192255   173 model_registry.cc:91] Model names: riva-trt-riva_text_classification_default-nn-bert-base-uncased, Model version: 1\n",
      "I0305 06:10:28.192875   173 model_registry.cc:91] Model names: riva_text_classification_default, Model version: 1\n",
      "I0305 06:10:28.193500   173 model_registry.cc:104] 'Successfully registering riva_text_classification_default'\n",
      "I0305 06:10:28.193526   173 model_registry.cc:91] Model names: riva_tokenizer, Model version: 1\n",
      "I0305 06:10:28.194119   173 model_registry.cc:109] Successfully registered: 1 models.\n",
      "I0305 06:10:28.194130   173 grpc_riva_nlp.cc:33] NLPService GRPC service started\n",
      "I0305 06:10:28.194145   173 client.cc:38] RivaLanguageUnderstandingClient initialized with server: localhost:8001\n",
      "I0305 06:10:28.194342   173 client.cc:54] Our model repository has: 3 models.\n",
      "I0305 06:10:28.195456   173 client.cc:72] Registering 'riva_text_classification_default' with service '/nvidia.riva.nlp.RivaLanguageUnderstanding/ClassifyText'\n",
      "I0305 06:10:28.196071   173 model_registry.cc:36] RivaModelRegistry initialized with server: localhost:8001\n",
      "I0305 06:10:28.196341   173 model_registry.cc:65] Server Name: triton, Server version: 2.9.0\n",
      "I0305 06:10:28.196523   173 model_registry.cc:86] Our model repository has a total of: 3 models\n",
      "I0305 06:10:28.196533   173 model_registry.cc:91] Model names: riva-trt-riva_text_classification_default-nn-bert-base-uncased, Model version: 1\n",
      "I0305 06:10:28.197115   173 model_registry.cc:91] Model names: riva_text_classification_default, Model version: 1\n",
      "I0305 06:10:28.197683   173 model_registry.cc:104] 'Successfully registering riva_text_classification_default'\n",
      "I0305 06:10:28.197710   173 model_registry.cc:91] Model names: riva_tokenizer, Model version: 1\n",
      "I0305 06:10:28.198300   173 model_registry.cc:109] Successfully registered: 1 models.\n",
      "I0305 06:10:28.198310   173 grpc_riva_nlp.cc:33] NLPService GRPC service started\n",
      "I0305 06:10:28.198558   173 riva_server.cc:91] NLP Service connected to Triton at localhost:8001\n",
      "I0305 06:10:28.198571   173 riva_server.cc:96] Riva Conversational AI Server listening on 0.0.0.0:50051\n"
     ]
    }
   ],
   "source": [
    "# Check Riva running services \n",
    "!docker logs riva-speech \\\n",
    "    2>&1|tee my_assessment/step6.txt # DO NOT REMOVE THIS LINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riva Service Request (not graded)\n",
    "Although the SST-2 data set is trained on movie sentiments, it will likely work in our restaurant domain too.  Give it a try with the following queries or make up your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run my_assessment/sentiment_analysis_client.py --query \"I like pizza\"\n",
    "%run my_assessment/sentiment_analysis_client.py --query \"I don't like this restaurant\"\n",
    "%run my_assessment/sentiment_analysis_client.py --query \"yeah, sounds good\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Riva Services "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down Riva \n",
    "!bash $RIVA_QS/riva_stop.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 7: Submit You Assessment\n",
    "How were your results? \n",
    "\n",
    "If you are satisfied that you have completed the code correctly, and that your training and deployment are correct, you can submit your project as follows to the autograder:\n",
    "\n",
    "1. Go back to the GPU launch page and click the checkmark to run the assessment:\n",
    "\n",
    "<img src=\"images/assess/assessment_checkmark.png\" width=800>\n",
    "\n",
    "2. That's it!  If you passed, you'll receive a pop-up window saying so, and the points will be credited to your progress.  If not, you'll receive feedback in the pop-up window. \n",
    "\n",
    "<img src=\"images/assess/assessment_pass_popup.png\" width=800>\n",
    "\n",
    "You can always check your assessment progress in the course progress tab.  Note that partial values for the coding assessment won't be visible here - it shows up as either 0 or 70 points.  Be sure to complete the questions on Transformer and Deployment on the same course page to qualify for your final certificate!\n",
    "\n",
    "<img src=\"images/assess/progress.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
